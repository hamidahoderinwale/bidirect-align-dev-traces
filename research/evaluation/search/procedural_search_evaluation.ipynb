{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedural Search Evaluation\n",
    "\n",
    "This notebook evaluates three search approaches for developer workflow patterns:\n",
    "1. **Baseline (Enhanced GitHub-style)**: Keyword matching + semantic embeddings on raw traces\n",
    "2. **Intent-Only Search**: Search over intent clusters (INTENT_DEBUG, INTENT_FEATURE, etc.)\n",
    "3. **Intent + Representation Search**: Multi-representation search combining intent and rungs (tokens, semantic_edits, functions, module_graph, motifs)\n",
    "\n",
    "## Methodology\n",
    "\n",
    "**Query Types:**\n",
    "- Procedural: Workflow patterns, temporal sequences\n",
    "- Functional: Function-level code patterns\n",
    "- Structural: Token patterns, edit operations\n",
    "- Module-level: File relationships, dependencies\n",
    "- Intent-driven: Semantic intent categories\n",
    "- Hybrid: Multi-representation queries\n",
    "- Context-aware: Project-specific similarity\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Standard IR: Precision@K, Recall@K, NDCG@K, MRR\n",
    "- Custom: Pattern Distinctness@K, Representation Diversity, Workflow Coherence\n",
    "\n",
    "**Ground Truth:** Three-stage process (embedding similarity → representation filtering → expert validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/hamidaho/new_cursor\n",
      "JSONL file: /Users/hamidaho/new_cursor/research/data/companion_traces.jsonl\n",
      "Results directory: /Users/hamidaho/new_cursor/research/results\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple, Set\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Search and similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Import rung extractors\n",
    "from rung_extractors import (\n",
    "    tokens_repr, semantic_edits_repr, functions_repr, module_graph_repr, motifs_repr,\n",
    "    canonicalize_prompt, event_sequence\n",
    ")\n",
    "\n",
    "# Find repo root\n",
    "def _find_repo_root() -> Path:\n",
    "    current = Path.cwd().resolve()\n",
    "    while True:\n",
    "        if (current / \"cursor-telemetry\").exists() or (current / \"components\").exists():\n",
    "            return current\n",
    "        if current == current.parent:\n",
    "            raise FileNotFoundError(\"Cannot locate repository root\")\n",
    "        current = current.parent\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "EXPORT_FILE_JSONL = REPO_ROOT / \"research/data/companion_traces.jsonl\"\n",
    "RESULTS_DIR = REPO_ROOT / \"research/results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"JSONL file: {EXPORT_FILE_JSONL}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading traces from companion_traces.jsonl...\n",
      "Loaded 160 traces (sessions)\n",
      "Total events: 5537\n",
      "\n",
      "Sample trace structure:\n",
      "  - Session ID: session-1761193218842\n",
      "  - Workspace: /Users/hamidaho\n",
      "  - Events: 2\n",
      "  - First event type: code_change\n",
      "  - First event details keys: ['file_path', 'diff_summary', 'diff_size', 'lines_added', 'lines_removed']\n"
     ]
    }
   ],
   "source": [
    "# Load traces from companion_traces.jsonl\n",
    "def load_traces_from_jsonl(file_path: Path, limit: Optional[int] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load traces from companion_traces.jsonl file.\n",
    "    \n",
    "    Structure: Each line is a session with:\n",
    "    - session_id\n",
    "    - workspace_path\n",
    "    - events: list of events (code_change, prompt, terminal_command, etc.)\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"JSONL file not found: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    sessions = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if limit and idx >= limit:\n",
    "                break\n",
    "            try:\n",
    "                session = json.loads(line.strip())\n",
    "                # Ensure events are sorted by timestamp\n",
    "                if 'events' in session:\n",
    "                    session['events'].sort(key=lambda x: x.get('timestamp', ''))\n",
    "                sessions.append(session)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return sessions\n",
    "\n",
    "# Load traces\n",
    "print(\"Loading traces from companion_traces.jsonl...\")\n",
    "traces = load_traces_from_jsonl(EXPORT_FILE_JSONL, limit=None)\n",
    "print(f\"Loaded {len(traces)} traces (sessions)\")\n",
    "print(f\"Total events: {sum(len(t.get('events', [])) for t in traces)}\")\n",
    "\n",
    "# Show sample structure\n",
    "if traces:\n",
    "    sample = traces[0]\n",
    "    print(f\"\\nSample trace structure:\")\n",
    "    print(f\"  - Session ID: {sample.get('session_id')}\")\n",
    "    print(f\"  - Workspace: {sample.get('workspace_path')}\")\n",
    "    print(f\"  - Events: {len(sample.get('events', []))}\")\n",
    "    if sample.get('events'):\n",
    "        print(f\"  - First event type: {sample['events'][0].get('type')}\")\n",
    "        print(f\"  - First event details keys: {list(sample['events'][0].get('details', {}).keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Representations for All Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating representations for all traces...\n",
      "This may take a while for large datasets...\n",
      "  Processing trace 0/160...\n",
      "  Processing trace 100/160...\n",
      "\n",
      "Generated representations for 160 traces\n",
      "Traces with intents: 0\n",
      "Traces with motifs: 144\n"
     ]
    }
   ],
   "source": [
    "# Generate representations for all traces\n",
    "print(\"Generating representations for all traces...\")\n",
    "print(\"This may take a while for large datasets...\")\n",
    "\n",
    "trace_representations = []\n",
    "for i, trace in enumerate(traces):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"  Processing trace {i}/{len(traces)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate all representation rungs\n",
    "        reprs = {\n",
    "            'session_id': trace.get('session_id'),\n",
    "            'workspace_path': trace.get('workspace_path'),\n",
    "            'tokens': tokens_repr(trace, include_prompts=True),\n",
    "            'semantic_edits': semantic_edits_repr(trace, include_prompts=True),\n",
    "            'functions': functions_repr(trace, include_prompts=True),\n",
    "            'module_graph': module_graph_repr(trace),\n",
    "            'motifs': motifs_repr(trace, use_statistical_mining=True, include_prompts=True),\n",
    "        }\n",
    "        \n",
    "        # Extract intents from trace\n",
    "        intents = []\n",
    "        for event in trace.get('events', []):\n",
    "            if event.get('type') == 'prompt':\n",
    "                prompt_text = event.get('details', {}).get('text', '')\n",
    "                if prompt_text:\n",
    "                    intent = canonicalize_prompt(prompt_text)\n",
    "                    intents.append(intent)\n",
    "        reprs['intents'] = list(set(intents))  # Unique intents\n",
    "        \n",
    "        # Create searchable text for baseline (enhanced GitHub-style)\n",
    "        searchable_text_parts = []\n",
    "        for event in trace.get('events', []):\n",
    "            # Add prompt text\n",
    "            if event.get('type') == 'prompt':\n",
    "                prompt_text = event.get('details', {}).get('text', '')\n",
    "                if prompt_text:\n",
    "                    searchable_text_parts.append(prompt_text)\n",
    "            # Add annotations\n",
    "            if event.get('annotation'):\n",
    "                searchable_text_parts.append(event.get('annotation'))\n",
    "            # Add file paths\n",
    "            file_path = event.get('details', {}).get('file_path') or event.get('details', {}).get('file')\n",
    "            if file_path:\n",
    "                searchable_text_parts.append(str(file_path))\n",
    "        reprs['searchable_text'] = ' '.join(searchable_text_parts)\n",
    "        \n",
    "        trace_representations.append(reprs)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing trace {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nGenerated representations for {len(trace_representations)} traces\")\n",
    "print(f\"Traces with intents: {sum(1 for r in trace_representations if r.get('intents'))}\")\n",
    "print(f\"Traces with motifs: {sum(1 for r in trace_representations if r.get('motifs'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Query Collection\n",
    "\n",
    "Sample query set (to be expanded with generated queries using the prompt from methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query set: 10 queries\n",
      "Query types: Counter({'procedural': 2, 'functional': 2, 'intent_driven': 2, 'structural': 1, 'module_level': 1, 'hybrid': 1, 'context_aware': 1})\n",
      "Difficulty levels: Counter({'medium': 4, 'hard': 4, 'easy': 2})\n"
     ]
    }
   ],
   "source": [
    "# Sample query set\n",
    "sample_queries = [\n",
    "    # Procedural queries\n",
    "    {'id': 'proc_1', 'text': 'Find workflows where developers debug an error then write a test', 'query_type': 'procedural', 'primary_intent': 'INTENT_DEBUG', 'target_rungs': ['motifs', 'semantic_edits'], 'difficulty': 'medium'},\n",
    "    {'id': 'proc_2', 'text': 'Show me patterns where feature creation is followed by documentation', 'query_type': 'procedural', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['motifs', 'semantic_edits'], 'difficulty': 'medium'},\n",
    "    # Functional queries\n",
    "    {'id': 'func_1', 'text': 'Find code that implements authentication with error handling', 'query_type': 'functional', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['functions', 'semantic_edits'], 'difficulty': 'medium'},\n",
    "    {'id': 'func_2', 'text': 'Show me patterns where create functions are followed by test functions', 'query_type': 'functional', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['functions', 'motifs'], 'difficulty': 'hard'},\n",
    "    # Structural queries\n",
    "    {'id': 'struct_1', 'text': 'Code with high complexity that was refactored', 'query_type': 'structural', 'primary_intent': 'INTENT_REFACTOR', 'target_rungs': ['tokens', 'semantic_edits'], 'difficulty': 'hard'},\n",
    "    # Module-level queries\n",
    "    {'id': 'module_1', 'text': 'Files that are typically edited together in authentication workflows', 'query_type': 'module_level', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['module_graph', 'motifs'], 'difficulty': 'medium'},\n",
    "    # Intent-driven queries\n",
    "    {'id': 'intent_1', 'text': 'Show me all debugging workflows', 'query_type': 'intent_driven', 'primary_intent': 'INTENT_DEBUG', 'target_rungs': ['intents'], 'difficulty': 'easy'},\n",
    "    {'id': 'intent_2', 'text': 'Feature development patterns', 'query_type': 'intent_driven', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['intents'], 'difficulty': 'easy'},\n",
    "    # Hybrid queries\n",
    "    {'id': 'hybrid_1', 'text': 'Debugging workflows that involve file switching with iterative patterns', 'query_type': 'hybrid', 'primary_intent': 'INTENT_DEBUG', 'target_rungs': ['intents', 'module_graph', 'motifs'], 'difficulty': 'hard'},\n",
    "    # Context-aware queries\n",
    "    {'id': 'context_1', 'text': 'Workflows similar to API endpoint development', 'query_type': 'context_aware', 'primary_intent': 'INTENT_FEATURE', 'target_rungs': ['module_graph', 'functions', 'motifs'], 'difficulty': 'hard'},\n",
    "]\n",
    "\n",
    "print(f\"Sample query set: {len(sample_queries)} queries\")\n",
    "print(f\"Query types: {Counter(q['query_type'] for q in sample_queries)}\")\n",
    "print(f\"Difficulty levels: {Counter(q['difficulty'] for q in sample_queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Ground Truth Creation\n",
    "\n",
    "Three-stage process: embedding similarity → representation filtering → (expert validation - manual step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No Hugging Face API key found. Set HF_TOKEN, HUGGINGFACE_API_KEY, or HF_API_KEY in your .env file. Get a free token at: https://huggingface.co/settings/tokens\n",
      "Ground truth creation will use representation-based filtering only\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model for ground truth (using Hugging Face)\n",
    "import requests\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings using Hugging Face API.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'sentence-transformers/all-mpnet-base-v2'):\n",
    "        self.model = model\n",
    "        self.hf_token = (\n",
    "            os.getenv('HF_TOKEN') or \n",
    "            os.getenv('HUGGINGFACE_API_KEY') or \n",
    "            os.getenv('HF_API_KEY') or \n",
    "            ''\n",
    "        ).strip()\n",
    "        \n",
    "        if not self.hf_token:\n",
    "            raise ValueError(\n",
    "                \"No Hugging Face API key found. Set HF_TOKEN, HUGGINGFACE_API_KEY, or HF_API_KEY \"\n",
    "                \"in your .env file. Get a free token at: https://huggingface.co/settings/tokens\"\n",
    "            )\n",
    "        \n",
    "        self.hf_endpoint = (\n",
    "            os.getenv('HF_EMBEDDING_ENDPOINT') or\n",
    "            f'https://api-inference.huggingface.co/pipeline/feature-extraction/{self.model}'\n",
    "        )\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        \n",
    "        response = requests.post(\n",
    "            self.hf_endpoint,\n",
    "            headers={\n",
    "                'Authorization': f'Bearer {self.hf_token}',\n",
    "                'Content-Type': 'application/json',\n",
    "            },\n",
    "            json={\n",
    "                'inputs': texts,\n",
    "                'options': {'wait_for_model': True}\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        embeddings = response.json()\n",
    "        \n",
    "        if isinstance(embeddings, list) and len(embeddings) > 0:\n",
    "            if isinstance(embeddings[0], list):\n",
    "                return np.array(embeddings)\n",
    "            else:\n",
    "                return np.array([embeddings])\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Initialize embedding generator\n",
    "try:\n",
    "    embedding_gen = EmbeddingGenerator()\n",
    "    print(\"Embedding generator initialized\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Ground truth creation will use representation-based filtering only\")\n",
    "    embedding_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ground truth candidates for queries...\n",
      "  proc_1: 0 relevant traces\n",
      "  proc_2: 0 relevant traces\n",
      "  func_1: 0 relevant traces\n",
      "  func_2: 0 relevant traces\n",
      "  struct_1: 0 relevant traces\n",
      "  module_1: 0 relevant traces\n",
      "  intent_1: 0 relevant traces\n",
      "  intent_2: 0 relevant traces\n",
      "  hybrid_1: 0 relevant traces\n",
      "  context_1: 0 relevant traces\n",
      "\n",
      "Ground truth generated for 10 queries\n",
      "Total relevant traces: 0\n",
      "Saved ground truth to: /Users/hamidaho/new_cursor/research/results/search_ground_truth.json\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Embedding-based candidate generation\n",
    "def generate_ground_truth_candidates(query: Dict, traces: List[Dict], trace_reprs: List[Dict], top_k: int = 50) -> List[int]:\n",
    "    \"\"\"Generate candidate traces for a query using embedding similarity.\"\"\"\n",
    "    if embedding_gen is None:\n",
    "        # Fallback: use representation-based only\n",
    "        return []\n",
    "    \n",
    "    # Embed query\n",
    "    query_embedding = embedding_gen.generate_embeddings([query['text']])[0]\n",
    "    \n",
    "    # Embed all trace searchable texts\n",
    "    trace_texts = [r.get('searchable_text', '') for r in trace_reprs]\n",
    "    trace_embeddings = embedding_gen.generate_embeddings(trace_texts)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity([query_embedding], trace_embeddings)[0]\n",
    "    \n",
    "    # Get top-K candidates\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    return top_indices.tolist()\n",
    "\n",
    "# Stage 2: Representation-based filtering\n",
    "def filter_by_representation(query: Dict, candidate_indices: List[int], trace_reprs: List[Dict]) -> List[int]:\n",
    "    \"\"\"Filter candidates using representation rungs.\"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    primary_intent = query.get('primary_intent')\n",
    "    target_rungs = query.get('target_rungs', [])\n",
    "    \n",
    "    for idx in candidate_indices:\n",
    "        trace_repr = trace_reprs[idx]\n",
    "        \n",
    "        # Check intent match\n",
    "        if primary_intent and primary_intent != 'mixed':\n",
    "            trace_intents = trace_repr.get('intents', [])\n",
    "            if primary_intent not in trace_intents:\n",
    "                continue  # Skip if intent doesn't match\n",
    "        \n",
    "        # Check rung match (at least one target rung should have content)\n",
    "        if target_rungs:\n",
    "            has_content = False\n",
    "            for rung in target_rungs:\n",
    "                if rung == 'intents':\n",
    "                    if trace_repr.get('intents'):\n",
    "                        has_content = True\n",
    "                        break\n",
    "                else:\n",
    "                    rung_content = trace_repr.get(rung, [])\n",
    "                    if rung_content and len(rung_content) > 0:\n",
    "                        has_content = True\n",
    "                        break\n",
    "            \n",
    "            if not has_content:\n",
    "                continue  # Skip if no relevant content\n",
    "        \n",
    "        filtered.append(idx)\n",
    "    \n",
    "    return filtered[:30]  # Return top 30 after filtering\n",
    "\n",
    "# Generate ground truth for all queries\n",
    "print(\"Generating ground truth candidates for queries...\")\n",
    "ground_truth = {}\n",
    "\n",
    "for query in sample_queries:\n",
    "    query_id = query['id']\n",
    "    \n",
    "    # Stage 1: Embedding-based candidates\n",
    "    candidates = generate_ground_truth_candidates(query, traces, trace_representations, top_k=50)\n",
    "    \n",
    "    # Stage 2: Representation-based filtering\n",
    "    if candidates:\n",
    "        filtered = filter_by_representation(query, candidates, trace_representations)\n",
    "    else:\n",
    "        # Fallback: use representation-based only\n",
    "        # Get all traces and filter by representation\n",
    "        all_indices = list(range(len(trace_representations)))\n",
    "        filtered = filter_by_representation(query, all_indices, trace_representations)\n",
    "    \n",
    "    ground_truth[query_id] = {\n",
    "        'query': query,\n",
    "        'relevant_trace_indices': filtered,\n",
    "        'n_relevant': len(filtered),\n",
    "        'note': 'Stage 3 (expert validation) should be performed manually'\n",
    "    }\n",
    "    \n",
    "    print(f\"  {query_id}: {len(filtered)} relevant traces\")\n",
    "\n",
    "print(f\"\\nGround truth generated for {len(ground_truth)} queries\")\n",
    "print(f\"Total relevant traces: {sum(gt['n_relevant'] for gt in ground_truth.values())}\")\n",
    "\n",
    "# Save ground truth\n",
    "gt_file = RESULTS_DIR / 'search_ground_truth.json'\n",
    "with open(gt_file, 'w') as f:\n",
    "    json.dump(ground_truth, f, indent=2, default=str)\n",
    "print(f\"Saved ground truth to: {gt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Search Implementations\n",
    "\n",
    "Implement three search conditions: Baseline (Enhanced GitHub-style), Intent-Only, Intent+Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline search implementation ready\n"
     ]
    }
   ],
   "source": [
    "# Search Implementation 1: Baseline (Enhanced GitHub-style)\n",
    "# Keyword matching + semantic embeddings\n",
    "\n",
    "def baseline_search(query: Dict, trace_reprs: List[Dict], top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Baseline search: Enhanced GitHub-style (keyword + semantic).\"\"\"\n",
    "    query_text = query['text']\n",
    "    \n",
    "    # Extract searchable texts\n",
    "    trace_texts = [r.get('searchable_text', '') for r in trace_reprs]\n",
    "    \n",
    "    # Keyword matching (TF-IDF)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(trace_texts)\n",
    "        query_vector = vectorizer.transform([query_text])\n",
    "        keyword_scores = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
    "    except:\n",
    "        keyword_scores = np.zeros(len(trace_texts))\n",
    "    \n",
    "    # Semantic similarity (embeddings)\n",
    "    if embedding_gen:\n",
    "        try:\n",
    "            query_embedding = embedding_gen.generate_embeddings([query_text])[0]\n",
    "            trace_embeddings = embedding_gen.generate_embeddings(trace_texts)\n",
    "            semantic_scores = cosine_similarity([query_embedding], trace_embeddings)[0]\n",
    "        except:\n",
    "            semantic_scores = np.zeros(len(trace_texts))\n",
    "    else:\n",
    "        semantic_scores = np.zeros(len(trace_texts))\n",
    "    \n",
    "    # Combine scores (weighted: 30% keyword, 70% semantic)\n",
    "    combined_scores = 0.3 * keyword_scores + 0.7 * semantic_scores\n",
    "    \n",
    "    # Get top-K\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    \n",
    "    return [(int(idx), float(combined_scores[idx])) for idx in top_indices]\n",
    "\n",
    "print(\"Baseline search implementation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent-only search implementation ready\n"
     ]
    }
   ],
   "source": [
    "# Search Implementation 2: Intent-Only Search\n",
    "\n",
    "def intent_only_search(query: Dict, trace_reprs: List[Dict], top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Intent-only search: Search over intent clusters only.\"\"\"\n",
    "    query_text = query['text']\n",
    "    primary_intent = query.get('primary_intent')\n",
    "    \n",
    "    # Extract intent from query if not provided\n",
    "    if not primary_intent or primary_intent == 'mixed':\n",
    "        query_intent = canonicalize_prompt(query_text)\n",
    "    else:\n",
    "        query_intent = primary_intent\n",
    "    \n",
    "    # Filter traces by intent match\n",
    "    intent_matches = []\n",
    "    for idx, trace_repr in enumerate(trace_reprs):\n",
    "        trace_intents = trace_repr.get('intents', [])\n",
    "        if query_intent in trace_intents:\n",
    "            # Score based on intent frequency (more intents = higher score)\n",
    "            score = trace_intents.count(query_intent) / max(len(trace_intents), 1)\n",
    "            intent_matches.append((idx, score))\n",
    "    \n",
    "    # Sort by score and return top-K\n",
    "    intent_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return intent_matches[:top_k]\n",
    "\n",
    "print(\"Intent-only search implementation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent + Representation search implementation ready\n"
     ]
    }
   ],
   "source": [
    "# Search Implementation 3: Intent + Representation Search\n",
    "\n",
    "def intent_representation_search(query: Dict, trace_reprs: List[Dict], top_k: int = 10, alpha: float = 0.3) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Intent + Representation search: Multi-rung search with intent filtering.\"\"\"\n",
    "    query_text = query['text']\n",
    "    primary_intent = query.get('primary_intent')\n",
    "    target_rungs = query.get('target_rungs', [])\n",
    "    \n",
    "    # Extract intent from query\n",
    "    if not primary_intent or primary_intent == 'mixed':\n",
    "        query_intent = canonicalize_prompt(query_text)\n",
    "    else:\n",
    "        query_intent = primary_intent\n",
    "    \n",
    "    # If no target rungs specified, use all rungs\n",
    "    if not target_rungs:\n",
    "        target_rungs = ['tokens', 'semantic_edits', 'functions', 'module_graph', 'motifs']\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for idx, trace_repr in enumerate(trace_reprs):\n",
    "        # Intent score\n",
    "        trace_intents = trace_repr.get('intents', [])\n",
    "        if query_intent in trace_intents:\n",
    "            intent_score = 1.0\n",
    "        else:\n",
    "            intent_score = 0.0\n",
    "        \n",
    "        # Rung scores (compute similarity for each target rung)\n",
    "        rung_scores = []\n",
    "        \n",
    "        for rung in target_rungs:\n",
    "            if rung == 'intents':\n",
    "                # Intent rung: exact match score\n",
    "                if query_intent in trace_intents:\n",
    "                    rung_scores.append(1.0)\n",
    "                else:\n",
    "                    rung_scores.append(0.0)\n",
    "            else:\n",
    "                # Other rungs: TF-IDF similarity\n",
    "                rung_content = trace_repr.get(rung, [])\n",
    "                if isinstance(rung_content, list):\n",
    "                    rung_text = ' '.join(str(item) for item in rung_content)\n",
    "                else:\n",
    "                    rung_text = str(rung_content)\n",
    "                \n",
    "                if rung_text:\n",
    "                    try:\n",
    "                        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "                        # Combine query and rung text for vectorization\n",
    "                        texts = [query_text, rung_text]\n",
    "                        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "                        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "                        rung_scores.append(similarity)\n",
    "                    except:\n",
    "                        rung_scores.append(0.0)\n",
    "                else:\n",
    "                    rung_scores.append(0.0)\n",
    "        \n",
    "        # Get max rung score\n",
    "        max_rung_score = max(rung_scores) if rung_scores else 0.0\n",
    "        \n",
    "        # Combined score: alpha * intent + (1-alpha) * max_rung\n",
    "        combined_score = alpha * intent_score + (1 - alpha) * max_rung_score\n",
    "        \n",
    "        scores.append((idx, combined_score))\n",
    "    \n",
    "    # Sort by score and return top-K\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scores[:top_k]\n",
    "\n",
    "print(\"Intent + Representation search implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Evaluation\n",
    "\n",
    "Run all queries through all three search conditions and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for all queries...\n",
      "  Skipping proc_1: No ground truth\n",
      "  Skipping proc_2: No ground truth\n",
      "  Skipping func_1: No ground truth\n",
      "  Skipping func_2: No ground truth\n",
      "  Skipping struct_1: No ground truth\n",
      "  Skipping module_1: No ground truth\n",
      "  Skipping intent_1: No ground truth\n",
      "  Skipping intent_2: No ground truth\n",
      "  Skipping hybrid_1: No ground truth\n",
      "  Skipping context_1: No ground truth\n",
      "\n",
      "Evaluation complete for 0 queries\n",
      "Saved evaluation results to: /Users/hamidaho/new_cursor/research/results/search_evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "def compute_metrics(predicted_indices: List[int], ground_truth_indices: List[int], k_values: List[int] = [1, 5, 10]) -> Dict[str, float]:\n",
    "    \"\"\"Compute standard IR metrics.\"\"\"\n",
    "    ground_truth_set = set(ground_truth_indices)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Precision@K, Recall@K\n",
    "    for k in k_values:\n",
    "        top_k_pred = set(predicted_indices[:k])\n",
    "        \n",
    "        if len(top_k_pred) > 0:\n",
    "            precision = len(top_k_pred & ground_truth_set) / len(top_k_pred)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        if len(ground_truth_set) > 0:\n",
    "            recall = len(top_k_pred & ground_truth_set) / len(ground_truth_set)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        \n",
    "        metrics[f'precision@{k}'] = precision\n",
    "        metrics[f'recall@{k}'] = recall\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0.0\n",
    "    for rank, idx in enumerate(predicted_indices, 1):\n",
    "        if idx in ground_truth_set:\n",
    "            mrr = 1.0 / rank\n",
    "            break\n",
    "    metrics['mrr'] = mrr\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running evaluation for all queries...\")\n",
    "evaluation_results = []\n",
    "\n",
    "for query in sample_queries:\n",
    "    query_id = query['id']\n",
    "    gt_indices = ground_truth[query_id]['relevant_trace_indices']\n",
    "    \n",
    "    if not gt_indices:\n",
    "        print(f\"  Skipping {query_id}: No ground truth\")\n",
    "        continue\n",
    "    \n",
    "    # Run all three search conditions\n",
    "    baseline_results = baseline_search(query, trace_representations, top_k=10)\n",
    "    intent_only_results = intent_only_search(query, trace_representations, top_k=10)\n",
    "    intent_repr_results = intent_representation_search(query, trace_representations, top_k=10)\n",
    "    \n",
    "    # Extract indices\n",
    "    baseline_indices = [idx for idx, score in baseline_results]\n",
    "    intent_only_indices = [idx for idx, score in intent_only_results]\n",
    "    intent_repr_indices = [idx for idx, score in intent_repr_results]\n",
    "    \n",
    "    # Compute metrics\n",
    "    baseline_metrics = compute_metrics(baseline_indices, gt_indices)\n",
    "    intent_only_metrics = compute_metrics(intent_only_indices, gt_indices)\n",
    "    intent_repr_metrics = compute_metrics(intent_repr_indices, gt_indices)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'query_id': query_id,\n",
    "        'query_text': query['text'],\n",
    "        'query_type': query['query_type'],\n",
    "        'difficulty': query['difficulty'],\n",
    "        'n_ground_truth': len(gt_indices),\n",
    "        'baseline': baseline_metrics,\n",
    "        'intent_only': intent_only_metrics,\n",
    "        'intent_representation': intent_repr_metrics,\n",
    "    })\n",
    "    \n",
    "    print(f\"  {query_id}: Baseline P@5={baseline_metrics['precision@5']:.3f}, \"\n",
    "          f\"Intent-Only P@5={intent_only_metrics['precision@5']:.3f}, \"\n",
    "          f\"Intent+Rep P@5={intent_repr_metrics['precision@5']:.3f}\")\n",
    "\n",
    "print(f\"\\nEvaluation complete for {len(evaluation_results)} queries\")\n",
    "\n",
    "# Save results\n",
    "results_file = RESULTS_DIR / 'search_evaluation_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, default=str)\n",
    "print(f\"Saved evaluation results to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AGGREGATE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Overall Averages:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'baseline_p5'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOverall Averages:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBaseline:        P@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbaseline_p5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mbaseline_r5\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, MRR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mbaseline_mrr\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIntent-Only:     P@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_only_p5\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_only_r5\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, MRR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_only_mrr\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIntent+Rep:      P@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_repr_p5\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_repr_r5\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, MRR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df[\u001b[33m'\u001b[39m\u001b[33mintent_repr_mrr\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_cursor/venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_cursor/venv/lib/python3.13/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'baseline_p5'"
     ]
    }
   ],
   "source": [
    "# Aggregate results and create summary\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'query_id': r['query_id'],\n",
    "        'query_type': r['query_type'],\n",
    "        'difficulty': r['difficulty'],\n",
    "        'baseline_p5': r['baseline']['precision@5'],\n",
    "        'baseline_r5': r['baseline']['recall@5'],\n",
    "        'baseline_mrr': r['baseline']['mrr'],\n",
    "        'intent_only_p5': r['intent_only']['precision@5'],\n",
    "        'intent_only_r5': r['intent_only']['recall@5'],\n",
    "        'intent_only_mrr': r['intent_only']['mrr'],\n",
    "        'intent_repr_p5': r['intent_representation']['precision@5'],\n",
    "        'intent_repr_r5': r['intent_representation']['recall@5'],\n",
    "        'intent_repr_mrr': r['intent_representation']['mrr'],\n",
    "    }\n",
    "    for r in evaluation_results\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nOverall Averages:\")\n",
    "print(f\"Baseline:        P@5={results_df['baseline_p5'].mean():.3f}, R@5={results_df['baseline_r5'].mean():.3f}, MRR={results_df['baseline_mrr'].mean():.3f}\")\n",
    "print(f\"Intent-Only:     P@5={results_df['intent_only_p5'].mean():.3f}, R@5={results_df['intent_only_r5'].mean():.3f}, MRR={results_df['intent_only_mrr'].mean():.3f}\")\n",
    "print(f\"Intent+Rep:      P@5={results_df['intent_repr_p5'].mean():.3f}, R@5={results_df['intent_repr_r5'].mean():.3f}, MRR={results_df['intent_repr_mrr'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nBy Query Type:\")\n",
    "for query_type in results_df['query_type'].unique():\n",
    "    type_df = results_df[results_df['query_type'] == query_type]\n",
    "    print(f\"\\n{query_type.upper()}:\")\n",
    "    print(f\"  Baseline:        P@5={type_df['baseline_p5'].mean():.3f}, R@5={type_df['baseline_r5'].mean():.3f}\")\n",
    "    print(f\"  Intent-Only:     P@5={type_df['intent_only_p5'].mean():.3f}, R@5={type_df['intent_only_r5'].mean():.3f}\")\n",
    "    print(f\"  Intent+Rep:      P@5={type_df['intent_repr_p5'].mean():.3f}, R@5={type_df['intent_repr_r5'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    diff_df = results_df[results_df['difficulty'] == difficulty]\n",
    "    if len(diff_df) > 0:\n",
    "        print(f\"\\n{difficulty.upper()}:\")\n",
    "        print(f\"  Baseline:        P@5={diff_df['baseline_p5'].mean():.3f}, R@5={diff_df['baseline_r5'].mean():.3f}\")\n",
    "        print(f\"  Intent-Only:     P@5={diff_df['intent_only_p5'].mean():.3f}, R@5={diff_df['intent_only_r5'].mean():.3f}\")\n",
    "        print(f\"  Intent+Rep:      P@5={diff_df['intent_repr_p5'].mean():.3f}, R@5={diff_df['intent_repr_r5'].mean():.3f}\")\n",
    "\n",
    "# Save summary\n",
    "summary_file = RESULTS_DIR / 'search_evaluation_summary.json'\n",
    "summary = {\n",
    "    'overall': {\n",
    "        'baseline': {\n",
    "            'precision@5': float(results_df['baseline_p5'].mean()),\n",
    "            'recall@5': float(results_df['baseline_r5'].mean()),\n",
    "            'mrr': float(results_df['baseline_mrr'].mean()),\n",
    "        },\n",
    "        'intent_only': {\n",
    "            'precision@5': float(results_df['intent_only_p5'].mean()),\n",
    "            'recall@5': float(results_df['intent_only_r5'].mean()),\n",
    "            'mrr': float(results_df['intent_only_mrr'].mean()),\n",
    "        },\n",
    "        'intent_representation': {\n",
    "            'precision@5': float(results_df['intent_repr_p5'].mean()),\n",
    "            'recall@5': float(results_df['intent_repr_r5'].mean()),\n",
    "            'mrr': float(results_df['intent_repr_mrr'].mean()),\n",
    "        },\n",
    "    },\n",
    "    'by_query_type': {},\n",
    "    'by_difficulty': {},\n",
    "}\n",
    "\n",
    "for query_type in results_df['query_type'].unique():\n",
    "    type_df = results_df[results_df['query_type'] == query_type]\n",
    "    summary['by_query_type'][query_type] = {\n",
    "        'baseline': {'precision@5': float(type_df['baseline_p5'].mean()), 'recall@5': float(type_df['baseline_r5'].mean())},\n",
    "        'intent_only': {'precision@5': float(type_df['intent_only_p5'].mean()), 'recall@5': float(type_df['intent_only_r5'].mean())},\n",
    "        'intent_representation': {'precision@5': float(type_df['intent_repr_p5'].mean()), 'recall@5': float(type_df['intent_repr_r5'].mean())},\n",
    "    }\n",
    "\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    diff_df = results_df[results_df['difficulty'] == difficulty]\n",
    "    if len(diff_df) > 0:\n",
    "        summary['by_difficulty'][difficulty] = {\n",
    "            'baseline': {'precision@5': float(diff_df['baseline_p5'].mean()), 'recall@5': float(diff_df['baseline_r5'].mean())},\n",
    "            'intent_only': {'precision@5': float(diff_df['intent_only_p5'].mean()), 'recall@5': float(diff_df['intent_only_r5'].mean())},\n",
    "            'intent_representation': {'precision@5': float(diff_df['intent_repr_p5'].mean()), 'recall@5': float(diff_df['intent_repr_r5'].mean())},\n",
    "        }\n",
    "\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSaved summary to: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

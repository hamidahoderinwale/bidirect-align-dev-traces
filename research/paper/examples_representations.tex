% Examples of Representation Encoders
% Suggested placement: After Section 5 (Transforming traces into useful representations)
% or as a new subsection within Section 5

\subsection{Representation Examples}
\label{subsec:examples}

To illustrate how each representation encoder transforms raw telemetry, we provide concrete examples from a single development session where a developer adds error handling to a data processing function.

\textbf{Raw Representation.} The raw representation preserves complete event logs with all metadata:

\begin{verbatim}
{
  "events": [
    {
      "type": "code_change",
      "timestamp": "2025-12-09T22:15:30.000Z",
      "file_path": "src/utils/error-handler.js",
      "language": "javascript",
      "diff": "@@ -15,3 +15,15 @@\n function processData(data) {\n+  try {\n+    return process(data);\n+  } catch (error) {\n+    logger.error('Processing failed', error);\n+    throw error;\n+  }\n }",
      "lines_added": 12,
      "lines_removed": 0,
      "session_id": "session_789ghi"
    },
    {
      "type": "prompt",
      "timestamp": "2025-12-09T22:15:00.000Z",
      "text": "Add error handling to the processData function",
      "model": "claude-3-opus",
      "context_files": ["src/utils/error-handler.js"]
    }
  ]
}
\end{verbatim}

\textbf{Tokens Representation.} Tokens normalize identifiers while preserving syntax structure:

\begin{verbatim}
[
  "FUNCTION", "IDENTIFIER_1", "LPAREN", "IDENTIFIER_2", "RPAREN", "LBRACE",
  "TRY", "LBRACE", "RETURN", "IDENTIFIER_3", "LPAREN", "IDENTIFIER_2", 
  "RPAREN", "SEMICOLON", "RBRACE", "CATCH", "LPAREN", "IDENTIFIER_4", 
  "RPAREN", "LBRACE", "IDENTIFIER_5", "DOT", "IDENTIFIER_6", "LPAREN", 
  "STRING_LITERAL", "COMMA", "IDENTIFIER_4", "RPAREN", "SEMICOLON", 
  "THROW", "IDENTIFIER_4", "SEMICOLON", "RBRACE", "RBRACE"
]
\end{verbatim}

\textbf{Semantic Edits Representation.} Semantic edits capture edit intent as operation-target pairs:

\begin{verbatim}
{
  "file_id": "FILE_a1b2c3d4",
  "edit_operations": [
    {
      "operation_type": "UPDATE_FUNCTION",
      "target": "FunctionDeclaration",
      "change_type": "modify"
    },
    {
      "operation_type": "INSERT_TRY",
      "target": "TryStatement",
      "change_type": "add"
    },
    {
      "operation_type": "INSERT_CATCH",
      "target": "CatchClause",
      "change_type": "add"
    }
  ],
  "intent": "error_handling",
  "lines_added": 12,
  "lines_removed": 0
}
\end{verbatim}

\textbf{Functions Representation.} Functions track API evolution at function granularity:

\begin{verbatim}
{
  "file_id": "FILE_a1b2c3d4",
  "functions_changed": [
    {
      "function_name": "processData",
      "change_type": "MODIFY",
      "signature": "processData(data)",
      "operations": ["ADD_TRY_CATCH", "ADD_ERROR_LOGGING"]
    }
  ],
  "api_evolution": {
    "functions_added": 0,
    "functions_modified": 1,
    "functions_removed": 0
  }
}
\end{verbatim}

\textbf{Module Graph Representation.} Module graphs encode file relationships and architectural coupling:

\begin{verbatim}
{
  "nodes": [
    {"file_id": "FILE_a1b2c3d4", "edits": 1, "navigations": 2},
    {"file_id": "FILE_e5f6g7h8", "edits": 0, "navigations": 1}
  ],
  "edges": [
    {
      "source": "FILE_a1b2c3d4",
      "target": "FILE_e5f6g7h8",
      "type": "imports",
      "weight": 0.8
    }
  ],
  "co_editing_patterns": {
    "FILE_a1b2c3d4": ["FILE_e5f6g7h8"]
  }
}
\end{verbatim}

\textbf{Motifs Representation.} Motifs combine temporal event sequences with semantic intent:

\begin{verbatim}
{
  "motifs": [
    "T_EV_a13f92_EV_10c99d",  // Transition: error-handler → logger
    "HOTSPOT_a13f92_3",        // Hotspot: 3 edits in error-handler
    "CYCLE_a13f_10c9_a13f"     // Cycle: error-handler → logger → error-handler
  ],
  "intent_category": "error_handling",
  "discovered_intent": "major refactoring with error handling",
  "procedural_pattern": "prompt → edit → test → error → edit",
  "compression_ratio": 242
}
\end{verbatim}

\textbf{Enrichment Pipeline.} The transformation from raw to motifs proceeds through multiple stages. Starting with raw events (13,712 bytes), normalization canonicalizes file paths and identifiers. Tokenization extracts syntax structure (2,341 bytes, 5.9× compression). AST parsing identifies semantic operations (892 bytes, 15.4× compression). Sequence mining discovers frequent patterns, and clustering assigns intent categories, producing motifs (57 bytes, 242× compression). Each stage preserves different signals: tokens preserve syntax, semantic edits preserve intent, functions preserve API structure, module graphs preserve architecture, and motifs preserve procedural patterns.


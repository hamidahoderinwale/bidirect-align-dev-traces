{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Probes Comparison\n",
    "\n",
    "This notebook extends the baseline probes (`probes_baseline.ipynb`) with advanced machine learning models and interpretability features.\n",
    "\n",
    "## Methods Included (ordered by efficiency):\n",
    "1. **TF-IDF + Logistic Regression** (Baseline) - Fastest, most interpretable\n",
    "2. **Pre-trained Embeddings + Logistic Regression** - Better semantic understanding\n",
    "3. **Fine-tuned Embeddings + Logistic Regression** (Optional) - Task-specific embeddings\n",
    "4. **Random Forest** - Non-linear patterns, interpretable trees\n",
    "5. **Gradient Boosting** - Strong performance, feature importance\n",
    "6. **Lightweight MLP** - Neural network baseline\n",
    "\n",
    "## Outputs:\n",
    "- All results saved to `research/results/advanced_probes_comparison.jsonl` (JSONL format)\n",
    "- Tree visualizations saved to `research/results/random_forest_trees.png`\n",
    "- Feature importance comparison saved to `research/results/feature_importance_comparison.png`\n",
    "- Method comparison summary saved to `research/results/method_comparison_summary.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:27.088914Z",
     "iopub.status.busy": "2025-12-28T05:57:27.088841Z",
     "iopub.status.idle": "2025-12-28T05:57:31.315055Z",
     "shell.execute_reply": "2025-12-28T05:57:31.314690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/hamidaho/new_cursor\n",
      "Trace export: /Users/hamidaho/new_cursor/research/data/companion_traces.jsonl\n",
      "Results directory: /Users/hamidaho/new_cursor/research/results\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path setup - MUST BE BEFORE ANY IMPORTS THAT USE REPO_ROOT\n",
    "REPO_ROOT = Path.cwd()\n",
    "while not (REPO_ROOT / 'cursor-telemetry').exists() and not (REPO_ROOT / 'research').exists():\n",
    "    if REPO_ROOT == REPO_ROOT.parent:\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# Add visualization utilities to path (notebook context)\n",
    "sys.path.insert(0, str(REPO_ROOT / \"research\" / \"scripts\"))\n",
    "from visualization import (\n",
    "    plot_bar, plot_scatter, plot_line, plot_heatmap, plot_pareto_frontier,\n",
    "    save_chart, RUNG_COLORS, RUNG_LABELS, COLOR_PALETTE\n",
    ")\n",
    "\n",
    "alt.renderers.enable('default')\n",
    "alt.data_transformers.enable('default')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_text, export_graphviz\n",
    "\n",
    "# Check for sentence-transformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    HAS_SENTENCE_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_SENTENCE_TRANSFORMERS = False\n",
    "    print(\"⚠ sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "\n",
    "    # Add research directory to path for imports\n",
    "    RESEARCH_DIR = REPO_ROOT / 'research'\n",
    "    RUNG_EXTRACTORS_DIR = RESEARCH_DIR / 'rung_extractors'\n",
    "    if str(RUNG_EXTRACTORS_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(RUNG_EXTRACTORS_DIR))\n",
    "    if str(RESEARCH_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(RESEARCH_DIR))\n",
    "    \n",
    "    from rung_extractors import (\n",
    "        tokens_repr_str,\n",
    "        semantic_edits_repr_str,\n",
    "        functions_repr_str,\n",
    "        motifs_repr_str,\n",
    "        raw_repr_str,\n",
    "    )\n",
    "\n",
    "# Data paths\n",
    "TRACE_EXPORT = REPO_ROOT / 'research/data/companion_traces.jsonl'\n",
    "RESULTS_DIR = REPO_ROOT / 'research/results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Trace export: {TRACE_EXPORT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:31.333140Z",
     "iopub.status.busy": "2025-12-28T05:57:31.332887Z",
     "iopub.status.idle": "2025-12-28T05:57:31.681502Z",
     "shell.execute_reply": "2025-12-28T05:57:31.681078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160 traces\n",
      "Loading cached rung extractions from /Users/hamidaho/new_cursor/research/results/rung_extractions_cache.pkl...\n",
      "✓ Using cached rung extractions\n",
      "Calculating label thresholds...\n",
      "Building final feature set...\n",
      "✓ Built features for 160 traces\n",
      "✓ Processing 4 rungs: tokens, semantic_edits, functions, motifs\n",
      "✓ Defined 4 classification tasks\n"
     ]
    }
   ],
   "source": [
    "# Load traces and build features\n",
    "if not TRACE_EXPORT.exists():\n",
    "    raise FileNotFoundError(f\"Trace export not found at {TRACE_EXPORT}. Run export script first.\")\n",
    "\n",
    "with TRACE_EXPORT.open('r', encoding='utf-8') as fh:\n",
    "    traces = [json.loads(line) for line in fh]\n",
    "\n",
    "print(f\"Loaded {len(traces)} traces\")\n",
    "\n",
    "# Rung extraction functions\n",
    "def tokens_repr(trace):\n",
    "    return tokens_repr_str(trace)\n",
    "\n",
    "def edits_repr(trace):\n",
    "    return semantic_edits_repr_str(trace)\n",
    "\n",
    "def functions_repr(trace):\n",
    "    return functions_repr_str(trace)\n",
    "\n",
    "def motifs_repr(trace):\n",
    "    return motifs_repr_str(trace)\n",
    "\n",
    "def raw_repr(trace):\n",
    "    return raw_repr_str(trace)\n",
    "\n",
    "RUNG_FUNCS = {\n",
    "    'tokens': tokens_repr,\n",
    "    'semantic_edits': edits_repr,\n",
    "    'functions': functions_repr,\n",
    "    'motifs': motifs_repr,\n",
    "    'raw': raw_repr,\n",
    "}\n",
    "\n",
    "# Feature extraction functions (using companion data)\n",
    "def count_code_changes(trace):\n",
    "    \"\"\"Count code/file change events in trace.\"\"\"\n",
    "    # Count entries (code changes) from companion data\n",
    "    count = len(trace.get('entries', []))\n",
    "    # Also count events\n",
    "    code_change_types = ['code_change', 'file_change', 'file_create', 'file_delete', 'file_rename', 'entry_created']\n",
    "    for event in trace.get('events', []):\n",
    "        event_type = (event.get('type') or '').lower()\n",
    "        if any(change_type in event_type for change_type in code_change_types):\n",
    "            count += 1\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict):\n",
    "            if details.get('after_content') or details.get('before_content') or details.get('code'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_unique_files(trace):\n",
    "    \"\"\"Count number of unique files modified in trace.\"\"\"\n",
    "    files = set()\n",
    "    # Check entries (code changes) from companion data\n",
    "    for entry in trace.get('entries', []):\n",
    "        file_path = entry.get('file_path')\n",
    "        if file_path:\n",
    "            files.add(str(file_path))\n",
    "    # Also check events\n",
    "    for event in trace.get('events', []):\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict):\n",
    "            file_path = details.get('file_path') or details.get('path')\n",
    "            if file_path:\n",
    "                files.add(str(file_path))\n",
    "    return len(files)\n",
    "\n",
    "def has_terminal_activity(trace):\n",
    "    \"\"\"Check if trace contains terminal commands.\"\"\"\n",
    "    # Check terminal_commands from companion data\n",
    "    if trace.get('terminal_commands') and len(trace.get('terminal_commands', [])) > 0:\n",
    "        return 1\n",
    "    # Also check events\n",
    "    for event in trace.get('events', []):\n",
    "        if 'terminal' in (event.get('type') or '').lower():\n",
    "            return 1\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict) and ('terminal' in str(details).lower() or 'command' in str(details).lower()):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def has_prompts(trace):\n",
    "    \"\"\"Check if trace contains prompts.\"\"\"\n",
    "    # Check prompts from companion data\n",
    "    if trace.get('prompts') and len(trace.get('prompts', [])) > 0:\n",
    "        return 1\n",
    "    # Also check events\n",
    "    for event in trace.get('events', []):\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict) and 'prompt' in details:\n",
    "            return 1\n",
    "        if event.get('prompt'):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Optimization: Skip raw rung (saves ~36 minutes) - uncomment if needed\n",
    "USE_RAW_RUNG = False  # Set to True if you need raw rung\n",
    "\n",
    "# Optimization: Cache rung extractions\n",
    "CACHE_FILE = RESULTS_DIR / 'rung_extractions_cache.pkl'\n",
    "USE_CACHE = True\n",
    "\n",
    "def process_single_trace(trace):\n",
    "    \"\"\"Process a single trace to extract features.\"\"\"\n",
    "    # Extract rung representations\n",
    "    rung_reps = {}\n",
    "    for rung_name, rung_func in RUNG_FUNCS.items():\n",
    "        # Skip raw if not needed (saves ~36 minutes)\n",
    "        if rung_name == 'raw' and not USE_RAW_RUNG:\n",
    "            continue\n",
    "        try:\n",
    "            rep = rung_func(trace)\n",
    "            rung_reps[rung_name] = rep if isinstance(rep, str) else ' '.join(rep) if isinstance(rep, list) else str(rep)\n",
    "        except Exception as e:\n",
    "            rung_reps[rung_name] = \"\"\n",
    "    \n",
    "    # Extract labels\n",
    "    code_changes = count_code_changes(trace)\n",
    "    unique_files = count_unique_files(trace)\n",
    "    has_terminal = has_terminal_activity(trace)\n",
    "    has_prompt = has_prompts(trace)\n",
    "    \n",
    "    return {\n",
    "        **rung_reps,\n",
    "        'code_changes': code_changes,\n",
    "        'unique_files': unique_files,\n",
    "        'has_terminal': has_terminal,\n",
    "        'has_prompt': has_prompt,\n",
    "    }\n",
    "\n",
    "# Check cache first\n",
    "if USE_CACHE and CACHE_FILE.exists():\n",
    "    print(f\"Loading cached rung extractions from {CACHE_FILE}...\")\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "            if len(cached_data) == len(traces):\n",
    "                print(\"✓ Using cached rung extractions\")\n",
    "                rung_extractions = cached_data\n",
    "            else:\n",
    "                print(f\"⚠ Cache size mismatch ({len(cached_data)} vs {len(traces)}), re-extracting...\")\n",
    "                rung_extractions = None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Cache load error: {e}, re-extracting...\")\n",
    "        rung_extractions = None\n",
    "else:\n",
    "    rung_extractions = None\n",
    "\n",
    "# Extract rungs (parallel or cached)\n",
    "if rung_extractions is None:\n",
    "    print(\"Building features (this may take a while)...\")\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # For programmatic execution (nbconvert), ALWAYS use sequential to avoid pickling errors\n",
    "    # Multiprocessing with notebook-defined functions fails in nbconvert due to pickling issues\n",
    "    # Even testing with Pool(1) can fail, so we skip parallel entirely for nbconvert\n",
    "    \n",
    "    # Check if we're in interactive Jupyter (only safe environment for multiprocessing)\n",
    "    USE_PARALLEL = False  # Default to sequential\n",
    "    \n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ipython = get_ipython()\n",
    "        # Only use parallel if we're in a real interactive Jupyter kernel\n",
    "        if ipython is not None and hasattr(ipython, 'kernel') and ipython.kernel is not None:\n",
    "            # Check if we're NOT in nbconvert (nbconvert doesn't have a real kernel)\n",
    "            import sys\n",
    "            if 'nbconvert' not in ' '.join(sys.argv).lower():\n",
    "                USE_PARALLEL = True\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "    \n",
    "    # Always use sequential for safety - parallel causes pickling errors in nbconvert\n",
    "    USE_PARALLEL = False  # Force sequential for programmatic execution\n",
    "    \n",
    "    print(\"Using sequential processing (required for programmatic execution)\")\n",
    "    rung_extractions = [process_single_trace(trace) for trace in traces]\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"✓ Extracted rungs in {elapsed:.2f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "    \n",
    "    # Save cache\n",
    "    if USE_CACHE:\n",
    "        print(f\"Saving cache to {CACHE_FILE}...\")\n",
    "        with open(CACHE_FILE, 'wb') as f:\n",
    "            pickle.dump(rung_extractions, f)\n",
    "        print(\"✓ Cache saved\")\n",
    "\n",
    "# Calculate median values for labels (needs all traces)\n",
    "print(\"Calculating label thresholds...\")\n",
    "all_code_changes = [r['code_changes'] for r in rung_extractions]\n",
    "all_unique_files = [r['unique_files'] for r in rung_extractions]\n",
    "median_code_changes = np.median(all_code_changes)\n",
    "median_unique_files = np.median(all_unique_files)\n",
    "\n",
    "# Build final features with labels\n",
    "print(\"Building final feature set...\")\n",
    "features = []\n",
    "for rung_data in rung_extractions:\n",
    "    feature_entry = {\n",
    "        **rung_data,\n",
    "        'high_code_activity': 1 if rung_data['code_changes'] > median_code_changes else 0,\n",
    "        'many_files': 1 if rung_data['unique_files'] > median_unique_files else 0,\n",
    "    }\n",
    "    features.append(feature_entry)\n",
    "\n",
    "print(f\"✓ Built features for {len(features)} traces\")\n",
    "\n",
    "# Define rungs to process (exclude raw if not needed) - used by all methods\n",
    "rungs_to_process = [r for r in RUNG_FUNCS.keys() if r != 'raw' or USE_RAW_RUNG]\n",
    "print(f\"✓ Processing {len(rungs_to_process)} rungs: {', '.join(rungs_to_process)}\")\n",
    "\n",
    "# Task labels for classification\n",
    "task_labels = [\n",
    "    ('high_code_activity', 'code_activity_classification'),\n",
    "    ('many_files', 'file_diversity_classification'),\n",
    "    ('has_terminal', 'terminal_usage_classification'),\n",
    "    ('has_prompt', 'prompt_usage_classification'),\n",
    "]\n",
    "\n",
    "print(f\"✓ Defined {len(task_labels)} classification tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: TF-IDF + Logistic Regression (Baseline)\n",
    "\n",
    "This is the baseline method from `probes_baseline.ipynb` - included for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:31.682971Z",
     "iopub.status.busy": "2025-12-28T05:57:31.682872Z",
     "iopub.status.idle": "2025-12-28T05:57:31.725124Z",
     "shell.execute_reply": "2025-12-28T05:57:31.724550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method 1 (TF-IDF + Logistic Regression) completed: 12 results\n"
     ]
    }
   ],
   "source": [
    "# Method 1: TF-IDF + Logistic Regression (Baseline)\n",
    "def build_tfidf_dataset(rung: str, label: str):\n",
    "    \"\"\"Build TF-IDF features for a given rung and label.\"\"\"\n",
    "    reps = [entry[rung] for entry in features]\n",
    "    placeholder = 'empty_repr'\n",
    "    cleaned = [repr if repr.strip() else placeholder for repr in reps]\n",
    "    vec = TfidfVectorizer(max_features=4096)\n",
    "    try:\n",
    "        X = vec.fit_transform(cleaned).toarray()\n",
    "    except ValueError:\n",
    "        X = np.zeros((len(cleaned), 1))\n",
    "    y = np.array([entry[label] for entry in features], dtype=int)\n",
    "    return X, y, vec\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Get rungs to process (exclude raw if not needed)\n",
    "rungs_to_process = [r for r in RUNG_FUNCS.keys() if r != 'raw' or USE_RAW_RUNG]\n",
    "\n",
    "for rung in rungs_to_process:\n",
    "    for label_name, task in task_labels:\n",
    "        X, y, vectorizer = build_tfidf_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Train/test split for generalization\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "        start = time.perf_counter()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.perf_counter() - start\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        infer_time = time.perf_counter() - start\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Model size (approximate)\n",
    "        model_size_mb = (clf.coef_.nbytes + clf.intercept_.nbytes) / (1024 * 1024)\n",
    "        \n",
    "        # Top features\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        coef_abs = np.abs(clf.coef_[0])\n",
    "        top_indices = np.argsort(coef_abs)[-15:][::-1]\n",
    "        top_features = {feature_names[i]: float(clf.coef_[0][i]) for i in top_indices}\n",
    "        \n",
    "        all_results.append({\n",
    "            'method': 'tfidf_logistic',\n",
    "            'rung': rung,\n",
    "            'task': task,\n",
    "            'train_accuracy': float(train_acc),\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'train_f1': float(train_f1),\n",
    "            'test_f1': float(test_f1),\n",
    "            'train_time': train_time,\n",
    "            'infer_time': infer_time,\n",
    "            'model_size_mb': model_size_mb,\n",
    "            'n_features': X.shape[1],\n",
    "            'top_features': top_features,\n",
    "            'generalization_gap': float(train_acc - test_acc)\n",
    "        })\n",
    "\n",
    "print(f\"✓ Method 1 (TF-IDF + Logistic Regression) completed: {len([r for r in all_results if r['method'] == 'tfidf_logistic'])} results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Pre-trained Embeddings + Logistic Regression\n",
    "\n",
    "Uses `sentence-transformers` to encode representations into dense embeddings, then trains Logistic Regression on top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:31.726874Z",
     "iopub.status.busy": "2025-12-28T05:57:31.726753Z",
     "iopub.status.idle": "2025-12-28T05:57:34.119818Z",
     "shell.execute_reply": "2025-12-28T05:57:34.119353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained embedding model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "  Encoding tokens / code_activity_classification...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Encoding tokens / file_diversity_classification...\n",
      "  Encoding tokens / terminal_usage_classification...\n",
      "  Encoding semantic_edits / code_activity_classification...\n",
      "  Encoding semantic_edits / file_diversity_classification...\n",
      "  Encoding semantic_edits / terminal_usage_classification...\n",
      "  Encoding functions / code_activity_classification...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Encoding functions / file_diversity_classification...\n",
      "  Encoding functions / terminal_usage_classification...\n",
      "  Encoding motifs / code_activity_classification...\n",
      "  Encoding motifs / file_diversity_classification...\n",
      "  Encoding motifs / terminal_usage_classification...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method 2 (Pre-trained Embeddings) completed: 12 results\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Pre-trained Embeddings + Logistic Regression\n",
    "if HAS_SENTENCE_TRANSFORMERS:\n",
    "    print(\"Loading pre-trained embedding model...\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, lightweight model\n",
    "    print(\"✓ Model loaded\")\n",
    "    \n",
    "    for rung in rungs_to_process:\n",
    "        for label_name, task in task_labels:\n",
    "            reps = [entry[rung] for entry in features]\n",
    "            y = np.array([entry[label_name] for entry in features], dtype=int)\n",
    "            if len(np.unique(y)) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Encode representations\n",
    "            print(f\"  Encoding {rung} / {task}...\")\n",
    "            start = time.perf_counter()\n",
    "            X = embedding_model.encode(reps, show_progress_bar=False)\n",
    "            encode_time = time.perf_counter() - start\n",
    "            \n",
    "            # Train/test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "            start = time.perf_counter()\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_time = time.perf_counter() - start\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            y_test_pred = clf.predict(X_test)\n",
    "            infer_time = time.perf_counter() - start\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            test_acc = accuracy_score(y_test, y_test_pred)\n",
    "            train_f1 = f1_score(y_train, y_train_pred)\n",
    "            test_f1 = f1_score(y_test, y_test_pred)\n",
    "            \n",
    "            model_size_mb = (clf.coef_.nbytes + clf.intercept_.nbytes) / (1024 * 1024)\n",
    "            \n",
    "            all_results.append({\n",
    "                'method': 'pretrained_embeddings',\n",
    "                'rung': rung,\n",
    "                'task': task,\n",
    "                'train_accuracy': float(train_acc),\n",
    "                'test_accuracy': float(test_acc),\n",
    "                'train_f1': float(train_f1),\n",
    "                'test_f1': float(test_f1),\n",
    "                'train_time': train_time + encode_time,\n",
    "                'infer_time': infer_time,\n",
    "                'model_size_mb': model_size_mb,\n",
    "                'embedding_dim': X.shape[1],\n",
    "                'generalization_gap': float(train_acc - test_acc)\n",
    "            })\n",
    "    \n",
    "    print(f\"✓ Method 2 (Pre-trained Embeddings) completed: {len([r for r in all_results if r['method'] == 'pretrained_embeddings'])} results\")\n",
    "else:\n",
    "    print(\"⚠ Skipping Method 2: sentence-transformers not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Random Forest\n",
    "\n",
    "Non-linear tree-based model with built-in feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:34.121370Z",
     "iopub.status.busy": "2025-12-28T05:57:34.121247Z",
     "iopub.status.idle": "2025-12-28T05:57:34.874246Z",
     "shell.execute_reply": "2025-12-28T05:57:34.873751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method 3 (Random Forest) completed: 12 results\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Random Forest\n",
    "for rung in rungs_to_process:\n",
    "    for label_name, task in task_labels:\n",
    "        X, y, vectorizer = build_tfidf_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        start = time.perf_counter()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.perf_counter() - start\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        infer_time = time.perf_counter() - start\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        importances = clf.feature_importances_\n",
    "        top_indices = np.argsort(importances)[-15:][::-1]\n",
    "        top_features = {feature_names[i]: float(importances[i]) for i in top_indices}\n",
    "        \n",
    "        all_results.append({\n",
    "            'method': 'random_forest',\n",
    "            'rung': rung,\n",
    "            'task': task,\n",
    "            'train_accuracy': float(train_acc),\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'train_f1': float(train_f1),\n",
    "            'test_f1': float(test_f1),\n",
    "            'train_time': train_time,\n",
    "            'infer_time': infer_time,\n",
    "            'n_estimators': 50,\n",
    "            'max_depth': 10,\n",
    "            'top_features': top_features,\n",
    "            'generalization_gap': float(train_acc - test_acc)\n",
    "        })\n",
    "\n",
    "print(f\"✓ Method 3 (Random Forest) completed: {len([r for r in all_results if r['method'] == 'random_forest'])} results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Gradient Boosting\n",
    "\n",
    "Sequential ensemble method that builds trees iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:34.875908Z",
     "iopub.status.busy": "2025-12-28T05:57:34.875809Z",
     "iopub.status.idle": "2025-12-28T05:57:34.988605Z",
     "shell.execute_reply": "2025-12-28T05:57:34.988256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method 4 (Gradient Boosting) completed: 12 results\n"
     ]
    }
   ],
   "source": [
    "# Method 4: Gradient Boosting\n",
    "for rung in rungs_to_process:\n",
    "    for label_name, task in task_labels:\n",
    "        X, y, vectorizer = build_tfidf_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        clf = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "        start = time.perf_counter()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.perf_counter() - start\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        infer_time = time.perf_counter() - start\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        importances = clf.feature_importances_\n",
    "        top_indices = np.argsort(importances)[-15:][::-1]\n",
    "        top_features = {feature_names[i]: float(importances[i]) for i in top_indices}\n",
    "        \n",
    "        all_results.append({\n",
    "            'method': 'gradient_boosting',\n",
    "            'rung': rung,\n",
    "            'task': task,\n",
    "            'train_accuracy': float(train_acc),\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'train_f1': float(train_f1),\n",
    "            'test_f1': float(test_f1),\n",
    "            'train_time': train_time,\n",
    "            'infer_time': infer_time,\n",
    "            'n_estimators': 50,\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 5,\n",
    "            'top_features': top_features,\n",
    "            'generalization_gap': float(train_acc - test_acc)\n",
    "        })\n",
    "\n",
    "print(f\"✓ Method 4 (Gradient Boosting) completed: {len([r for r in all_results if r['method'] == 'gradient_boosting'])} results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Lightweight MLP\n",
    "\n",
    "Small neural network with early stopping for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:34.989845Z",
     "iopub.status.busy": "2025-12-28T05:57:34.989761Z",
     "iopub.status.idle": "2025-12-28T05:57:35.065469Z",
     "shell.execute_reply": "2025-12-28T05:57:35.065036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Method 5 (MLP) completed: 12 results\n"
     ]
    }
   ],
   "source": [
    "# Method 5: Lightweight MLP\n",
    "for rung in rungs_to_process:\n",
    "    for label_name, task in task_labels:\n",
    "        X, y, vectorizer = build_tfidf_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=(64,),\n",
    "            activation='relu',\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        start = time.perf_counter()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.perf_counter() - start\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        infer_time = time.perf_counter() - start\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        \n",
    "        all_results.append({\n",
    "            'method': 'mlp',\n",
    "            'rung': rung,\n",
    "            'task': task,\n",
    "            'train_accuracy': float(train_acc),\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'train_f1': float(train_f1),\n",
    "            'test_f1': float(test_f1),\n",
    "            'train_time': train_time,\n",
    "            'infer_time': infer_time,\n",
    "            'hidden_layer_sizes': (64,),\n",
    "            'n_iterations': clf.n_iter_,\n",
    "            'generalization_gap': float(train_acc - test_acc)\n",
    "        })\n",
    "\n",
    "print(f\"✓ Method 5 (MLP) completed: {len([r for r in all_results if r['method'] == 'mlp'])} results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Visualizations\n",
    "\n",
    "Visualize Random Forest trees for interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:35.067010Z",
     "iopub.status.busy": "2025-12-28T05:57:35.066909Z",
     "iopub.status.idle": "2025-12-28T05:57:35.104803Z",
     "shell.execute_reply": "2025-12-28T05:57:35.104307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ graphviz Python package installed, but system package not found.\n",
      "   Install system package: brew install graphviz (macOS) or apt-get install graphviz (Linux)\n",
      "   Falling back to text representation.\n",
      "\n",
      "======================================================================\n",
      "Random Forest Trees: motifs / code_activity_classification\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Tree 1 (Text Representation):\n",
      "======================================================================\n",
      "|--- class: 0.0\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Tree 2 (Text Representation):\n",
      "======================================================================\n",
      "|--- class: 0.0\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Tree 3 (Text Representation):\n",
      "======================================================================\n",
      "|--- class: 0.0\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Detailed Text Representation of First Tree:\n",
      "======================================================================\n",
      "|--- class: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Tree Visualizations (for a specific rung/task combination)\n",
    "# Uses graphviz for tree visualization (alternative to matplotlib)\n",
    "\n",
    "try:\n",
    "    from graphviz import Source\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import subprocess\n",
    "    # Check if graphviz system package is installed\n",
    "    try:\n",
    "        subprocess.run(['dot', '-V'], capture_output=True, check=True)\n",
    "        HAS_GRAPHVIZ = True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        HAS_GRAPHVIZ = False\n",
    "        print(\"⚠ graphviz Python package installed, but system package not found.\")\n",
    "        print(\"   Install system package: brew install graphviz (macOS) or apt-get install graphviz (Linux)\")\n",
    "        print(\"   Falling back to text representation.\")\n",
    "except ImportError:\n",
    "    HAS_GRAPHVIZ = False\n",
    "    print(\"⚠ graphviz not available. Install with: pip install graphviz\")\n",
    "    print(\"   Also install system package: brew install graphviz (macOS) or apt-get install graphviz (Linux)\")\n",
    "    print(\"   Falling back to text representation.\")\n",
    "\n",
    "rung_viz = 'motifs'\n",
    "task_viz = 'code_activity_classification'\n",
    "label_viz = 'high_code_activity'\n",
    "\n",
    "X, y, vectorizer = build_tfidf_dataset(rung_viz, label_viz)\n",
    "if len(np.unique(y)) >= 2:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Small forest for visualization\n",
    "    clf_viz = RandomForestClassifier(n_estimators=3, max_depth=4, random_state=42)\n",
    "    clf_viz.fit(X_train, y_train)\n",
    "    \n",
    "    # Visualize trees using graphviz\n",
    "    if HAS_GRAPHVIZ:\n",
    "        feature_names = list(vectorizer.get_feature_names_out()[:20])\n",
    "        class_names = [str(i) for i in range(len(np.unique(y_train)))]\n",
    "        \n",
    "        # Create tree visualizations\n",
    "        tree_graphs = []\n",
    "        for i, tree in enumerate(clf_viz.estimators_):\n",
    "            # Export tree to graphviz format\n",
    "            dot_data = export_graphviz(\n",
    "                tree,\n",
    "                out_file=None,\n",
    "                feature_names=feature_names,\n",
    "                class_names=class_names,\n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True,\n",
    "                max_depth=3,\n",
    "                fontsize=10\n",
    "            )\n",
    "            \n",
    "            # Create graphviz Source object\n",
    "            graph = Source(dot_data)\n",
    "            \n",
    "            # Save as PNG\n",
    "            output_path = RESULTS_DIR / f'random_forest_tree_{i+1}.png'\n",
    "            graph.render(filename=str(output_path.with_suffix('')), format='png', cleanup=True)\n",
    "            print(f\"✓ Saved tree {i+1} visualization to {output_path}\")\n",
    "            \n",
    "            tree_graphs.append(graph)\n",
    "        \n",
    "        # Display trees in notebook\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Random Forest Trees: {rung_viz} / {task_viz}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        for i, graph in enumerate(tree_graphs):\n",
    "            print(f\"\\nTree {i+1}:\")\n",
    "            print(\"-\" * 70)\n",
    "            graph  # Display in notebook\n",
    "        \n",
    "        # Also save combined visualization if possible\n",
    "        print(f\"\\n✓ All tree visualizations saved to {RESULTS_DIR}\")\n",
    "    else:\n",
    "        # Fallback: Enhanced text representation\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Random Forest Trees: {rung_viz} / {task_viz}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        feature_names = list(vectorizer.get_feature_names_out()[:20])\n",
    "        for i, tree in enumerate(clf_viz.estimators_):\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Tree {i+1} (Text Representation):\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(export_text(tree, feature_names=feature_names, max_depth=3))\n",
    "            print()\n",
    "    \n",
    "    # Text representation of first tree (always show)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Detailed Text Representation of First Tree:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    feature_names = list(vectorizer.get_feature_names_out()[:20])\n",
    "    print(export_text(clf_viz.estimators_[0], feature_names=feature_names, max_depth=3))\n",
    "else:\n",
    "    print(f\"⚠ Cannot visualize: insufficient classes for {rung_viz} / {task_viz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Comparison\n",
    "\n",
    "Compare top features across different methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:35.106219Z",
     "iopub.status.busy": "2025-12-28T05:57:35.106101Z",
     "iopub.status.idle": "2025-12-28T05:57:35.571233Z",
     "shell.execute_reply": "2025-12-28T05:57:35.570795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved feature importance comparison to /Users/hamidaho/new_cursor/research/results/feature_importance_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Comparison\n",
    "# Compare top features for a specific rung/task\n",
    "rung_comp = 'motifs'\n",
    "task_comp = 'code_activity_classification'\n",
    "label_comp = 'high_code_activity'\n",
    "\n",
    "comparison_data = []\n",
    "for result in all_results:\n",
    "    if result['rung'] == rung_comp and result['task'] == task_comp and 'top_features' in result:\n",
    "        method = result['method']\n",
    "        top_features = result['top_features']\n",
    "        for feature, importance in list(top_features.items())[:15]:\n",
    "            comparison_data.append({\n",
    "                'method': method,\n",
    "                'feature': feature,\n",
    "                'importance': importance\n",
    "            })\n",
    "\n",
    "if comparison_data:\n",
    "    comp_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Get top 15 features per method\n",
    "    methods = comp_df['method'].unique()\n",
    "    \n",
    "    # Create charts for each method\n",
    "    charts = []\n",
    "    for method in methods:\n",
    "        method_data = comp_df[comp_df['method'] == method].nlargest(15, 'importance').sort_values('importance')\n",
    "        chart = alt.Chart(method_data).mark_bar(opacity=0.7, cornerRadius=4).encode(\n",
    "            x=alt.X('importance:Q', title='Importance', axis=alt.Axis(titleFontWeight='bold')),\n",
    "            y=alt.Y('feature:N', title='Feature', sort=alt.SortField('importance', order='ascending'), \n",
    "                   axis=alt.Axis(titleFontWeight='bold', labelFontSize=9)),\n",
    "            tooltip=['feature', alt.Tooltip('importance:Q', format='.4f')],\n",
    "        ).properties(\n",
    "            width=400,\n",
    "            height=300,\n",
    "            title=alt.TitleParams(text=f'{method.replace(\"_\", \" \").title()}: Top 15 Features', \n",
    "                                fontWeight='bold', fontSize=14)\n",
    "        )\n",
    "        # Don't configure individual charts - will configure the dashboard\n",
    "        charts.append(chart)\n",
    "    \n",
    "    # Combine charts vertically\n",
    "    if len(charts) > 1:\n",
    "        combined = alt.vconcat(*charts, spacing=30).configure_axis(gridOpacity=0.3)\n",
    "    else:\n",
    "        combined = charts[0].configure_axis(gridOpacity=0.3)\n",
    "    \n",
    "    # Save chart\n",
    "    save_chart(combined, RESULTS_DIR / 'feature_importance_comparison.png', scale_factor=2.0)\n",
    "    print(f\"✓ Saved feature importance comparison to {RESULTS_DIR / 'feature_importance_comparison.png'}\")\n",
    "    \n",
    "    combined\n",
    "else:\n",
    "    print(\"⚠ No feature importance data available for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Comparison: Performance & Efficiency\n",
    "\n",
    "Compare all methods across accuracy, training time, model size, and generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T05:57:35.572679Z",
     "iopub.status.busy": "2025-12-28T05:57:35.572565Z",
     "iopub.status.idle": "2025-12-28T05:57:35.856422Z",
     "shell.execute_reply": "2025-12-28T05:57:35.856014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved all results to /Users/hamidaho/new_cursor/research/results/advanced_probes_comparison.jsonl\n",
      "\n",
      "================================================================================\n",
      "METHOD COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Grouped by method:\n",
      "                      test_accuracy         test_f1         train_time  \\\n",
      "                               mean     std    mean     std       mean   \n",
      "method                                                                   \n",
      "gradient_boosting            0.5521  0.0769  0.4864  0.3632     0.0065   \n",
      "mlp                          0.5521  0.0769  0.7086  0.0619     0.0036   \n",
      "pretrained_embeddings        0.5521  0.0769  0.2642  0.3902     0.0548   \n",
      "random_forest                0.5521  0.0769  0.2642  0.3902     0.0317   \n",
      "tfidf_logistic               0.5521  0.0769  0.2642  0.3902     0.0007   \n",
      "\n",
      "                              generalization_gap          \n",
      "                          std               mean     std  \n",
      "method                                                    \n",
      "gradient_boosting      0.0006             0.0078  0.0067  \n",
      "mlp                    0.0004            -0.0026  0.0102  \n",
      "pretrained_embeddings  0.0614             0.0078  0.0067  \n",
      "random_forest          0.0017             0.0078  0.0067  \n",
      "tfidf_logistic         0.0004             0.0078  0.0067  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved method comparison to /Users/hamidaho/new_cursor/research/results/method_comparison_summary.png\n",
      "\n",
      "================================================================================\n",
      "BEST METHODS BY METRIC\n",
      "================================================================================\n",
      "Best Test Accuracy: tfidf_logistic (0.6562)\n",
      "Fastest Training: tfidf_logistic (0.0005s)\n",
      "Smallest Model: tfidf_logistic (0.0000 MB)\n",
      "Best Generalization (lowest gap): mlp (-0.0156)\n"
     ]
    }
   ],
   "source": [
    "# Method Comparison: Performance & Efficiency\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Save all results to JSONL\n",
    "output_file = RESULTS_DIR / 'advanced_probes_comparison.jsonl'\n",
    "with output_file.open('w') as f:\n",
    "    for result in all_results:\n",
    "        # Convert numpy types to native Python types for JSON\n",
    "        json_result = json.loads(json.dumps(result, default=str))\n",
    "        f.write(json.dumps(json_result) + '\\n')\n",
    "print(f\"✓ Saved all results to {output_file}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGrouped by method:\")\n",
    "summary = df_all.groupby('method').agg({\n",
    "    'test_accuracy': ['mean', 'std'],\n",
    "    'test_f1': ['mean', 'std'],\n",
    "    'train_time': ['mean', 'std'],\n",
    "    'generalization_gap': ['mean', 'std']\n",
    "}).round(4)\n",
    "print(summary)\n",
    "\n",
    "# Visualization: 4-panel comparison using Altair\n",
    "# 1. Test Accuracy\n",
    "method_acc = df_all.groupby('method')['test_accuracy'].mean().sort_values(ascending=False).reset_index()\n",
    "method_acc['method_label'] = method_acc['method'].str.replace('_', ' ').str.title()\n",
    "chart1 = alt.Chart(method_acc).mark_bar(opacity=0.7, cornerRadius=4, color=COLOR_PALETTE[0]).encode(\n",
    "    x=alt.X('method_label:N', title='Method', sort=alt.SortField('test_accuracy', order='descending'),\n",
    "           axis=alt.Axis(labelAngle=-45, titleFontWeight='bold')),\n",
    "    y=alt.Y('test_accuracy:Q', title='Mean Test Accuracy', axis=alt.Axis(titleFontWeight='bold')),\n",
    "    tooltip=['method_label', alt.Tooltip('test_accuracy:Q', format='.3f')],\n",
    ").properties(\n",
    "    width=300, height=250,\n",
    "    title=alt.TitleParams(text='Test Accuracy by Method', fontWeight='bold', fontSize=14)\n",
    ")\n",
    "# Don't configure individual charts - will configure the dashboard\n",
    "\n",
    "# 2. Training Time\n",
    "method_time = df_all.groupby('method')['train_time'].mean().sort_values(ascending=True).reset_index()\n",
    "method_time['method_label'] = method_time['method'].str.replace('_', ' ').str.title()\n",
    "chart2 = alt.Chart(method_time).mark_bar(opacity=0.7, cornerRadius=4, color=COLOR_PALETTE[1]).encode(\n",
    "    x=alt.X('train_time:Q', title='Mean Training Time (seconds)', axis=alt.Axis(titleFontWeight='bold')),\n",
    "    y=alt.Y('method_label:N', title='Method', sort=alt.SortField('train_time', order='ascending'),\n",
    "           axis=alt.Axis(titleFontWeight='bold')),\n",
    "    tooltip=['method_label', alt.Tooltip('train_time:Q', format='.4f')],\n",
    ").properties(\n",
    "    width=300, height=250,\n",
    "    title=alt.TitleParams(text='Training Time by Method', fontWeight='bold', fontSize=14)\n",
    ")\n",
    "# Don't configure individual charts - will configure the dashboard\n",
    "\n",
    "# 3. Model Size (where available)\n",
    "if 'model_size_mb' in df_all.columns:\n",
    "    method_size = df_all[df_all['model_size_mb'].notna()].groupby('method')['model_size_mb'].mean().sort_values(ascending=True).reset_index()\n",
    "    if not method_size.empty:\n",
    "        method_size['method_label'] = method_size['method'].str.replace('_', ' ').str.title()\n",
    "        chart3 = alt.Chart(method_size).mark_bar(opacity=0.7, cornerRadius=4, color=COLOR_PALETTE[2]).encode(\n",
    "            x=alt.X('model_size_mb:Q', title='Mean Model Size (MB)', axis=alt.Axis(titleFontWeight='bold')),\n",
    "            y=alt.Y('method_label:N', title='Method', sort=alt.SortField('model_size_mb', order='ascending'),\n",
    "                   axis=alt.Axis(titleFontWeight='bold')),\n",
    "            tooltip=['method_label', alt.Tooltip('model_size_mb:Q', format='.4f')],\n",
    "        ).properties(\n",
    "            width=300, height=250,\n",
    "            title=alt.TitleParams(text='Model Size by Method', fontWeight='bold', fontSize=14)\n",
    "        )\n",
    "        # Don't configure individual charts - will configure the dashboard\n",
    "    else:\n",
    "        chart3 = alt.Chart(pd.DataFrame({'text': ['Model size data not available']})).mark_text(\n",
    "            fontSize=14\n",
    "        ).encode(text='text:N').properties(\n",
    "            width=300, height=250,\n",
    "            title=alt.TitleParams(text='Model Size by Method', fontWeight='bold', fontSize=14)\n",
    "        )\n",
    "else:\n",
    "    chart3 = alt.Chart(pd.DataFrame({'text': ['Model size data not available']})).mark_text(\n",
    "        fontSize=14\n",
    "    ).encode(text='text:N').properties(\n",
    "        width=300, height=250,\n",
    "        title=alt.TitleParams(text='Model Size by Method', fontWeight='bold', fontSize=14)\n",
    "    )\n",
    "\n",
    "# 4. Generalization Gap\n",
    "method_gap = df_all.groupby('method')['generalization_gap'].mean().sort_values(ascending=True).reset_index()\n",
    "method_gap['method_label'] = method_gap['method'].str.replace('_', ' ').str.title()\n",
    "chart4 = alt.Chart(method_gap).mark_bar(opacity=0.7, cornerRadius=4, color=COLOR_PALETTE[3]).encode(\n",
    "    x=alt.X('generalization_gap:Q', title='Mean Generalization Gap (Train - Test)', \n",
    "           axis=alt.Axis(titleFontWeight='bold')),\n",
    "    y=alt.Y('method_label:N', title='Method', sort=alt.SortField('generalization_gap', order='ascending'),\n",
    "           axis=alt.Axis(titleFontWeight='bold')),\n",
    "    tooltip=['method_label', alt.Tooltip('generalization_gap:Q', format='.4f')],\n",
    ").properties(\n",
    "    width=300, height=250,\n",
    "    title=alt.TitleParams(text='Overfitting Measure (Lower = Better)', fontWeight='bold', fontSize=14)\n",
    ")\n",
    "# Don't configure individual charts - will configure the dashboard\n",
    "\n",
    "# Combine into dashboard\n",
    "top_row = alt.hconcat(chart1, chart2, spacing=20)\n",
    "bottom_row = alt.hconcat(chart3, chart4, spacing=20)\n",
    "dashboard = alt.vconcat(top_row, bottom_row, spacing=30).configure(\n",
    "    view={'continuousWidth': 300, 'continuousHeight': 250},\n",
    "    axis={'labelFontSize': 11, 'titleFontSize': 12, 'titleFontWeight': 'bold', 'gridOpacity': 0.3},\n",
    "    title={'fontSize': 16, 'fontWeight': 'bold'},\n",
    ").properties(\n",
    "    title=alt.TitleParams(text='Advanced Probes: Method Comparison', fontWeight='bold', fontSize=16)\n",
    ")\n",
    "\n",
    "# Save chart\n",
    "save_chart(dashboard, RESULTS_DIR / 'method_comparison_summary.png', scale_factor=2.0)\n",
    "print(f\"\\n✓ Saved method comparison to {RESULTS_DIR / 'method_comparison_summary.png'}\")\n",
    "\n",
    "dashboard\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST METHODS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Test Accuracy: {df_all.loc[df_all['test_accuracy'].idxmax(), 'method']} ({df_all['test_accuracy'].max():.4f})\")\n",
    "print(f\"Fastest Training: {df_all.loc[df_all['train_time'].idxmin(), 'method']} ({df_all['train_time'].min():.4f}s)\")\n",
    "if 'model_size_mb' in df_all.columns:\n",
    "    size_data = df_all[df_all['model_size_mb'].notna()]\n",
    "    if not size_data.empty:\n",
    "        print(f\"Smallest Model: {size_data.loc[size_data['model_size_mb'].idxmin(), 'method']} ({size_data['model_size_mb'].min():.4f} MB)\")\n",
    "print(f\"Best Generalization (lowest gap): {df_all.loc[df_all['generalization_gap'].idxmin(), 'method']} ({df_all['generalization_gap'].min():.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

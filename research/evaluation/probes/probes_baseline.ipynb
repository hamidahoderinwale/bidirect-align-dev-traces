{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe Baselines \n",
    "Train probes across next-event prediction, context retrieval, session classification, and anomaly detection while capturing storage/compute costs per rung. Results go in `research/results/probe_metrics.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd64d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using REPO_ROOT: /Users/hamidaho/new_cursor\n",
      "Trace file exists: /Users/hamidaho/new_cursor/research/data/companion_traces.jsonl\n",
      "Loaded 40238 traces\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Find repo root - the directory containing research/rung_extractors\n",
    "# Walk up from current directory until we find research/rung_extractors\n",
    "current = Path.cwd()\n",
    "REPO_ROOT = None\n",
    "\n",
    "# Walk up the directory tree to find repo root\n",
    "while current != current.parent:\n",
    "    # Check if this directory contains research/rung_extractors (the actual repo root)\n",
    "    research_dir = current / 'research'\n",
    "    rung_extractors = research_dir / 'rung_extractors'\n",
    "    if research_dir.exists() and rung_extractors.exists():\n",
    "        REPO_ROOT = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "# Fallback: if not found, try to find any research directory\n",
    "if REPO_ROOT is None:\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'research').exists():\n",
    "            REPO_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "# Final fallback: use current directory\n",
    "if REPO_ROOT is None:\n",
    "    REPO_ROOT = Path.cwd()\n",
    "\n",
    "print(f\"Using REPO_ROOT: {REPO_ROOT}\")\n",
    "\n",
    "# Import shared rung extraction functions\n",
    "RESEARCH_DIR = REPO_ROOT / 'research'\n",
    "RUNG_EXTRACTORS_DIR = RESEARCH_DIR / 'rung_extractors'\n",
    "if str(RUNG_EXTRACTORS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(RUNG_EXTRACTORS_DIR))\n",
    "if str(RESEARCH_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(RESEARCH_DIR))\n",
    "\n",
    "from rung_extractors import (\n",
    "    tokens_repr_str,\n",
    "    semantic_edits_repr_str,\n",
    "    functions_repr_str,\n",
    "    motifs_repr_str,\n",
    "    raw_repr_str,\n",
    ")\n",
    "TRACE_EXPORT = REPO_ROOT / 'research/data/companion_traces.jsonl'\n",
    "TRACE_EXPORT.parent.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR = REPO_ROOT / 'research/results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export traces if needed (use Python script if available, otherwise skip if file exists)\n",
    "def export_traces() -> None:\n",
    "    # Use the exact path to the export script\n",
    "    EXPORT_SCRIPT = REPO_ROOT / 'research/scripts/export_complete_traces.py'\n",
    "    \n",
    "    if not EXPORT_SCRIPT.exists():\n",
    "        # If script doesn't exist, check if trace file already exists\n",
    "        if TRACE_EXPORT.exists():\n",
    "            print(f\"Trace file already exists at {TRACE_EXPORT}, skipping export\")\n",
    "            return\n",
    "        raise FileNotFoundError(\n",
    "            f'Export script not found at {EXPORT_SCRIPT} and trace file does not exist at {TRACE_EXPORT}'\n",
    "        )\n",
    "    \n",
    "    print(f\"Running export script: {EXPORT_SCRIPT}\")\n",
    "    subprocess.run([sys.executable, str(EXPORT_SCRIPT)], check=True, cwd=str(REPO_ROOT))\n",
    "\n",
    "if not TRACE_EXPORT.exists():\n",
    "    print(f\"Trace file not found at {TRACE_EXPORT}. Exporting traces...\")\n",
    "    export_traces()\n",
    "else:\n",
    "    print(f\"Trace file exists: {TRACE_EXPORT}\")\n",
    "\n",
    "# Load traces\n",
    "if not TRACE_EXPORT.exists():\n",
    "    raise FileNotFoundError(f\"Trace file still does not exist after export attempt: {TRACE_EXPORT}\")\n",
    "\n",
    "with TRACE_EXPORT.open('r', encoding='utf-8') as fh:\n",
    "    traces = [json.loads(line) for line in fh]\n",
    "print(f'Loaded {len(traces)} traces')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee111101",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49004c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using shared rung extraction functions from rung_extractors.py\n",
      "  Available rungs: ['tokens', 'semantic_edits', 'functions', 'motifs', 'raw']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 40238 feature rows\n",
      "\n",
      "============================================================\n",
      "Label Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "next_code_change:\n",
      "  Positive: 164 (0.4%)\n",
      "  Negative: 40074 (99.6%)\n",
      "  Unique values: {0, 1}\n",
      "  ⚠ WARNING: Severe class imbalance (<10% minority class)\n",
      "\n",
      "high_code_activity:\n",
      "  Positive: 164 (0.4%)\n",
      "  Negative: 40074 (99.6%)\n",
      "  Unique values: {0, 1}\n",
      "  ⚠ WARNING: Severe class imbalance (<10% minority class)\n",
      "\n",
      "multi_file_session:\n",
      "  Positive: 164 (0.4%)\n",
      "  Negative: 40074 (99.6%)\n",
      "  Unique values: {0, 1}\n",
      "  ⚠ WARNING: Severe class imbalance (<10% minority class)\n",
      "\n",
      "has_terminal:\n",
      "  Positive: 0 (0.0%)\n",
      "  Negative: 40238 (100.0%)\n",
      "  Unique values: {0}\n",
      "  ⚠ WARNING: Only one class present - will skip classification\n",
      "\n",
      "has_prompts:\n",
      "  Positive: 1 (0.0%)\n",
      "  Negative: 40237 (100.0%)\n",
      "  Unique values: {0, 1}\n",
      "  ⚠ WARNING: Severe class imbalance (<10% minority class)\n",
      "\n",
      "anomaly:\n",
      "  Positive: 164 (0.4%)\n",
      "  Negative: 40074 (99.6%)\n",
      "  Unique values: {0, 1}\n",
      "  ⚠ WARNING: Severe class imbalance (<10% minority class)\n",
      "\n",
      "Most common event types (last event in trace):\n",
      "  none: 40074\n",
      "  code_change: 164\n",
      "\n",
      "Most common event types (all events):\n",
      "  code_change: 5549\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Rung Extraction Functions\n",
    "# ============================================================================\n",
    "# All rung extraction functions are imported from rung_extractors.py\n",
    "# This ensures consistency across all notebooks and provides a single source of truth.\n",
    "#\n",
    "# Wrapper functions to match the expected interface (return strings)\n",
    "def tokens_repr(trace):\n",
    "    \"\"\"Use shared tokens_repr_str from rung_extractors.\"\"\"\n",
    "    return tokens_repr_str(trace)\n",
    "\n",
    "def edits_repr(trace):\n",
    "    \"\"\"Use shared semantic_edits_repr_str from rung_extractors.\"\"\"\n",
    "    return semantic_edits_repr_str(trace)\n",
    "\n",
    "def functions_repr(trace):\n",
    "    \"\"\"Use shared functions_repr_str from rung_extractors.\"\"\"\n",
    "    return functions_repr_str(trace)\n",
    "\n",
    "def motifs_repr(trace):\n",
    "    \"\"\"Use shared motifs_repr_str from rung_extractors.\"\"\"\n",
    "    return motifs_repr_str(trace)\n",
    "\n",
    "\n",
    "def raw_repr(trace):\n",
    "    \"\"\"Use shared raw_repr_str from rung_extractors.\"\"\"\n",
    "    return raw_repr_str(trace)\n",
    "\n",
    "RUNG_FUNCS = {\n",
    "    'tokens': tokens_repr,\n",
    "    'semantic_edits': edits_repr,\n",
    "    'functions': functions_repr,\n",
    "    'motifs': motifs_repr,\n",
    "    'raw': raw_repr,\n",
    "}\n",
    "\n",
    "print(\"✓ Using shared rung extraction functions from rung_extractors.py\")\n",
    "print(f\"  Available rungs: {list(RUNG_FUNCS.keys())}\")\n",
    "def label_from_trace(trace):\n",
    "    events = trace.get('events', [])\n",
    "    if not events:\n",
    "        return 'none'\n",
    "    return events[-1].get('type') or 'unknown'\n",
    "\n",
    "# Improved label creation logic - creates balanced labels based on trace characteristics\n",
    "def count_code_changes(trace):\n",
    "    \"\"\"Count code/file change events in trace.\"\"\"\n",
    "    code_change_types = [\n",
    "        'code_change', 'file_change', 'file_create', 'file_delete', \n",
    "        'file_rename', 'entry_created'\n",
    "    ]\n",
    "    count = 0\n",
    "    for event in trace.get('events', []):\n",
    "        event_type = (event.get('type') or '').lower()\n",
    "        if any(change_type in event_type for change_type in code_change_types):\n",
    "            count += 1\n",
    "        # Also check details for code content\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict):\n",
    "            if details.get('after_content') or details.get('before_content') or details.get('code'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_unique_files(trace):\n",
    "    \"\"\"Count number of unique files modified in trace.\"\"\"\n",
    "    files = set()\n",
    "    for event in trace.get('events', []):\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict):\n",
    "            file_path = details.get('file_path') or details.get('file')\n",
    "            if file_path:\n",
    "                files.add(str(file_path))\n",
    "    return len(files)\n",
    "\n",
    "def has_terminal_commands(trace):\n",
    "    \"\"\"Check if trace contains terminal commands.\"\"\"\n",
    "    for event in trace.get('events', []):\n",
    "        event_type = (event.get('type') or '').lower()\n",
    "        if 'terminal' in event_type or 'command' in event_type:\n",
    "            return True\n",
    "        details = event.get('details', {})\n",
    "        if isinstance(details, dict) and details.get('command'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_prompts(trace):\n",
    "    \"\"\"Check if trace contains prompts/AI interactions.\"\"\"\n",
    "    for event in trace.get('events', []):\n",
    "        event_type = (event.get('type') or '').lower()\n",
    "        if 'prompt' in event_type or 'conversation' in event_type:\n",
    "            return True\n",
    "        if event.get('ai_generated'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def last_event_is_code_change(trace):\n",
    "    \"\"\"Check if last event is a code change (for next_event prediction).\"\"\"\n",
    "    events = trace.get('events', [])\n",
    "    if not events:\n",
    "        return False\n",
    "    last_event = events[-1]\n",
    "    event_type = (last_event.get('type') or '').lower()\n",
    "    code_types = ['code_change', 'file_change', 'entry_created']\n",
    "    return any(code_type in event_type for code_type in code_types)\n",
    "\n",
    "# Calculate cutoffs for balanced labels\n",
    "event_counts = [len(trace.get('events', [])) for trace in traces]\n",
    "code_change_counts = [count_code_changes(trace) for trace in traces]\n",
    "file_counts = [count_unique_files(trace) for trace in traces]\n",
    "\n",
    "anomaly_cutoff = np.percentile(event_counts, 90) if event_counts else 0\n",
    "high_code_activity_cutoff = np.percentile(code_change_counts, 50) if code_change_counts else 0  # Median split\n",
    "multi_file_cutoff = np.percentile(file_counts, 50) if file_counts else 1  # Median split\n",
    "\n",
    "features = []\n",
    "for trace in traces:\n",
    "    entry = {}\n",
    "    for name, fn in RUNG_FUNCS.items():\n",
    "        entry[name] = fn(trace)\n",
    "    \n",
    "    entry['label'] = label_from_trace(trace)\n",
    "    \n",
    "    # Improved label creation - creates balanced binary labels\n",
    "    # 1. next_code_change: Check if last event is a code change (for next-event prediction)\n",
    "    entry['next_code_change'] = int(last_event_is_code_change(trace))\n",
    "    \n",
    "    # 2. high_code_activity: Traces with above-median code change count\n",
    "    code_change_count = count_code_changes(trace)\n",
    "    entry['high_code_activity'] = int(code_change_count > high_code_activity_cutoff)\n",
    "    \n",
    "    # 3. multi_file_session: Traces that modify multiple files (above median)\n",
    "    unique_files = count_unique_files(trace)\n",
    "    entry['multi_file_session'] = int(unique_files > multi_file_cutoff)\n",
    "    \n",
    "    # 4. has_terminal: Traces with terminal commands\n",
    "    entry['has_terminal'] = int(has_terminal_commands(trace))\n",
    "    \n",
    "    # 5. has_prompts: Traces with AI prompts/interactions\n",
    "    entry['has_prompts'] = int(has_prompts(trace))\n",
    "    \n",
    "    # 6. anomaly: Based on event count (top 10%)\n",
    "    entry['anomaly'] = int(len(trace.get('events', [])) > anomaly_cutoff)\n",
    "    \n",
    "    # Target file for retrieval\n",
    "    entry['target_file'] = next(\n",
    "        (event.get('details', {}).get('file_path') or event.get('details', {}).get('file') \n",
    "         for event in reversed(trace.get('events', [])) if event.get('details')), \n",
    "        trace.get('workspace_path', 'unknown')\n",
    "    )\n",
    "    \n",
    "    # Storage and term counts\n",
    "    entry['storage_bytes'] = {name: len(entry[name].encode('utf-8')) for name in RUNG_FUNCS}\n",
    "    entry['term_count'] = {name: len(entry[name].split()) for name in RUNG_FUNCS}\n",
    "    \n",
    "    features.append(entry)\n",
    "print('Built {} feature rows'.format(len(features)))\n",
    "\n",
    "# Debug: Check label distributions\n",
    "print('\\n' + '='*60)\n",
    "print('Label Distribution Analysis')\n",
    "print('='*60)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Check all label distributions\n",
    "label_names = [\n",
    "    'next_code_change', 'high_code_activity', 'multi_file_session', \n",
    "    'has_terminal', 'has_prompts', 'anomaly'\n",
    "]\n",
    "for label_name in label_names:\n",
    "    counts = [entry[label_name] for entry in features]\n",
    "    positive = sum(counts)\n",
    "    negative = len(counts) - positive\n",
    "    print(f'\\n{label_name}:')\n",
    "    print(f'  Positive: {positive} ({positive/len(counts)*100:.1f}%)')\n",
    "    print(f'  Negative: {negative} ({negative/len(counts)*100:.1f}%)')\n",
    "    print(f'  Unique values: {set(counts)}')\n",
    "    if positive == 0 or negative == 0:\n",
    "        print(f'  ⚠ WARNING: Only one class present - will skip classification')\n",
    "    elif positive < len(counts) * 0.1 or negative < len(counts) * 0.1:\n",
    "        print(f'  ⚠ WARNING: Severe class imbalance (<10% minority class)')\n",
    "\n",
    "# Check what labels are actually being generated\n",
    "label_types = [entry['label'] for entry in features]\n",
    "label_counter = Counter(label_types)\n",
    "print(f'\\nMost common event types (last event in trace):')\n",
    "for label, count in label_counter.most_common(10):\n",
    "    print(f'  {label}: {count}')\n",
    "\n",
    "# Check event types in traces\n",
    "all_event_types = []\n",
    "for trace in traces:\n",
    "    for event in trace.get('events', []):\n",
    "        event_type = event.get('type', '').lower()\n",
    "        if event_type:\n",
    "            all_event_types.append(event_type)\n",
    "event_type_counter = Counter(all_event_types)\n",
    "print(f'\\nMost common event types (all events):')\n",
    "for etype, count in event_type_counter.most_common(10):\n",
    "    print(f'  {etype}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c4ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping tokens / has_terminal because only one class present\n",
      "Skipping semantic_edits / has_terminal because only one class present\n",
      "Skipping functions / has_terminal because only one class present\n",
      "Skipping motifs / has_terminal because only one class present\n",
      "Skipping raw / has_terminal because only one class present\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rung</th>\n",
       "      <th>task</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>train_time</th>\n",
       "      <th>infer_time</th>\n",
       "      <th>storage_bytes</th>\n",
       "      <th>avg_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokens</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.340506</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokens</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.120972</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tokens</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.162775</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tokens</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190184</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tokens</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.197283</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>next_event</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.451204</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.467872</td>\n",
       "      <td>0.028396</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.654524</td>\n",
       "      <td>0.063071</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.054430</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888593</td>\n",
       "      <td>0.067836</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>functions</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.233073</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>functions</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.201618</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>functions</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.141306</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>functions</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218365</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>functions</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.246301</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>motifs</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>motifs</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.118419</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>motifs</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.365719</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>motifs</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098018</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>motifs</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.157488</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>raw</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.544939</td>\n",
       "      <td>0.164612</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>raw</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.648676</td>\n",
       "      <td>0.127795</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>raw</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.717060</td>\n",
       "      <td>0.100346</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>raw</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.703527</td>\n",
       "      <td>0.091919</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>raw</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.747743</td>\n",
       "      <td>0.119874</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rung                          task  accuracy        f1  \\\n",
       "0           tokens                    next_event  0.999950  0.993865   \n",
       "1           tokens  code_activity_classification  0.999950  0.993865   \n",
       "2           tokens     multi_file_classification  0.999950  0.993865   \n",
       "3           tokens         prompt_classification  0.999975  0.000000   \n",
       "4           tokens             anomaly_detection  0.999950  0.993865   \n",
       "5   semantic_edits                    next_event  1.000000  1.000000   \n",
       "6   semantic_edits  code_activity_classification  1.000000  1.000000   \n",
       "7   semantic_edits     multi_file_classification  1.000000  1.000000   \n",
       "8   semantic_edits         prompt_classification  0.999975  0.000000   \n",
       "9   semantic_edits             anomaly_detection  1.000000  1.000000   \n",
       "10       functions                    next_event  0.999080  0.872852   \n",
       "11       functions  code_activity_classification  0.999080  0.872852   \n",
       "12       functions     multi_file_classification  0.999080  0.872852   \n",
       "13       functions         prompt_classification  0.999975  0.000000   \n",
       "14       functions             anomaly_detection  0.999080  0.872852   \n",
       "15          motifs                    next_event  0.999602  0.948718   \n",
       "16          motifs  code_activity_classification  0.999602  0.948718   \n",
       "17          motifs     multi_file_classification  0.999602  0.948718   \n",
       "18          motifs         prompt_classification  0.999975  0.000000   \n",
       "19          motifs             anomaly_detection  0.999602  0.948718   \n",
       "20             raw                    next_event  0.999677  0.958730   \n",
       "21             raw  code_activity_classification  0.999677  0.958730   \n",
       "22             raw     multi_file_classification  0.999677  0.958730   \n",
       "23             raw         prompt_classification  0.999975  0.000000   \n",
       "24             raw             anomaly_detection  0.999677  0.958730   \n",
       "\n",
       "    train_time  infer_time  storage_bytes  avg_terms  \n",
       "0     0.340506    0.008164      16.693374   1.818207  \n",
       "1     0.120972    0.009219      16.693374   1.818207  \n",
       "2     0.162775    0.006980      16.693374   1.818207  \n",
       "3     0.190184    0.023912      16.693374   1.818207  \n",
       "4     0.197283    0.013617      16.693374   1.818207  \n",
       "5     0.451204    0.025396      15.896739   1.148765  \n",
       "6     0.467872    0.028396      15.896739   1.148765  \n",
       "7     0.654524    0.063071      15.896739   1.148765  \n",
       "8     0.495498    0.054430      15.896739   1.148765  \n",
       "9     0.888593    0.067836      15.896739   1.148765  \n",
       "10    0.233073    0.012403      12.371539   1.135991  \n",
       "11    0.201618    0.012878      12.371539   1.135991  \n",
       "12    0.141306    0.012997      12.371539   1.135991  \n",
       "13    0.218365    0.029164      12.371539   1.135991  \n",
       "14    0.246301    0.012389      12.371539   1.135991  \n",
       "15    0.111183    0.007152      14.278866   1.035290  \n",
       "16    0.118419    0.005044      14.278866   1.035290  \n",
       "17    0.365719    0.018551      14.278866   1.035290  \n",
       "18    0.098018    0.007322      14.278866   1.035290  \n",
       "19    0.157488    0.009276      14.278866   1.035290  \n",
       "20    1.544939    0.164612     641.943238  74.039540  \n",
       "21    1.648676    0.127795     641.943238  74.039540  \n",
       "22    1.717060    0.100346     641.943238  74.039540  \n",
       "23    1.703527    0.091919     641.943238  74.039540  \n",
       "24    1.747743    0.119874     641.943238  74.039540  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_probe_dataset(rung: str, label: str):\n",
    "    reps = [entry[rung] for entry in features]\n",
    "    placeholder='empty_repr'\n",
    "    cleaned=[repr if repr.strip() else placeholder for repr in reps]\n",
    "    vec = TfidfVectorizer(max_features=4096)\n",
    "    # Check if we have any non-empty representations\n",
    "    if cleaned and any(rep.strip() for rep in cleaned):\n",
    "        X = vec.fit_transform(cleaned).toarray()\n",
    "    else:\n",
    "        X = np.zeros((len(cleaned), 1))\n",
    "    y = np.array([entry[label] for entry in features], dtype=int)\n",
    "    return X, y\n",
    "results = []\n",
    "# Updated task labels - balanced labels based on trace characteristics\n",
    "task_labels = [\n",
    "    ('next_code_change', 'next_event'),\n",
    "    ('high_code_activity', 'code_activity_classification'),\n",
    "    ('multi_file_session', 'multi_file_classification'),\n",
    "    ('has_terminal', 'terminal_classification'),\n",
    "    ('has_prompts', 'prompt_classification'),\n",
    "    ('anomaly', 'anomaly_detection')\n",
    "]\n",
    "\n",
    "for rung in RUNG_FUNCS:\n",
    "    for label_name, task in task_labels:\n",
    "        X, y = build_probe_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            print(f'Skipping {rung} / {label_name} because only one class present')\n",
    "            continue\n",
    "        clf = LogisticRegression(max_iter=500)\n",
    "        start = time.perf_counter()\n",
    "        clf.fit(X, y)\n",
    "        train_time = time.perf_counter() - start\n",
    "        start = time.perf_counter()\n",
    "        y_pred = clf.predict(X)\n",
    "        infer_time = time.perf_counter() - start\n",
    "        results.append({\n",
    "            'rung': rung,\n",
    "            'task': task,\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'f1': f1_score(y, y_pred),\n",
    "            'train_time': train_time,\n",
    "            'infer_time': infer_time,\n",
    "            'storage_bytes': np.mean([entry['storage_bytes'][rung] for entry in features]),\n",
    "            'avg_terms': np.mean([entry['term_count'][rung] for entry in features])\n",
    "        })\n",
    "df_probes = pd.DataFrame(results)\n",
    "df_probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d80507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rung</th>\n",
       "      <th>task</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>train_time</th>\n",
       "      <th>infer_time</th>\n",
       "      <th>storage_bytes</th>\n",
       "      <th>avg_terms</th>\n",
       "      <th>recall@1</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokens</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.340506</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokens</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.120972</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tokens</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.162775</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tokens</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190184</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tokens</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.197283</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>next_event</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.451204</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.467872</td>\n",
       "      <td>0.028396</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.654524</td>\n",
       "      <td>0.063071</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.054430</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888593</td>\n",
       "      <td>0.067836</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>functions</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.233073</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>functions</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.201618</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>functions</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.141306</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>functions</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218365</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>functions</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.246301</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>motifs</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.111183</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>motifs</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.118419</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>motifs</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.365719</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>motifs</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098018</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>motifs</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.157488</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>raw</td>\n",
       "      <td>next_event</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.544939</td>\n",
       "      <td>0.164612</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>raw</td>\n",
       "      <td>code_activity_classification</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.648676</td>\n",
       "      <td>0.127795</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>raw</td>\n",
       "      <td>multi_file_classification</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.717060</td>\n",
       "      <td>0.100346</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>raw</td>\n",
       "      <td>prompt_classification</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.703527</td>\n",
       "      <td>0.091919</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>raw</td>\n",
       "      <td>anomaly_detection</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.958730</td>\n",
       "      <td>1.747743</td>\n",
       "      <td>0.119874</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tokens</td>\n",
       "      <td>context_retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.693374</td>\n",
       "      <td>1.818207</td>\n",
       "      <td>0.996173</td>\n",
       "      <td>0.996695</td>\n",
       "      <td>0.997043</td>\n",
       "      <td>46.699182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>semantic_edits</td>\n",
       "      <td>context_retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.896739</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.996744</td>\n",
       "      <td>0.997490</td>\n",
       "      <td>0.997639</td>\n",
       "      <td>64.151296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>functions</td>\n",
       "      <td>context_retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.371539</td>\n",
       "      <td>1.135991</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.997217</td>\n",
       "      <td>0.997341</td>\n",
       "      <td>86.988718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>motifs</td>\n",
       "      <td>context_retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.278866</td>\n",
       "      <td>1.035290</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.996695</td>\n",
       "      <td>0.996993</td>\n",
       "      <td>102.346322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>raw</td>\n",
       "      <td>context_retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>641.943238</td>\n",
       "      <td>74.039540</td>\n",
       "      <td>0.996720</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>361.740176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rung                          task  accuracy        f1  \\\n",
       "0           tokens                    next_event  0.999950  0.993865   \n",
       "1           tokens  code_activity_classification  0.999950  0.993865   \n",
       "2           tokens     multi_file_classification  0.999950  0.993865   \n",
       "3           tokens         prompt_classification  0.999975  0.000000   \n",
       "4           tokens             anomaly_detection  0.999950  0.993865   \n",
       "5   semantic_edits                    next_event  1.000000  1.000000   \n",
       "6   semantic_edits  code_activity_classification  1.000000  1.000000   \n",
       "7   semantic_edits     multi_file_classification  1.000000  1.000000   \n",
       "8   semantic_edits         prompt_classification  0.999975  0.000000   \n",
       "9   semantic_edits             anomaly_detection  1.000000  1.000000   \n",
       "10       functions                    next_event  0.999080  0.872852   \n",
       "11       functions  code_activity_classification  0.999080  0.872852   \n",
       "12       functions     multi_file_classification  0.999080  0.872852   \n",
       "13       functions         prompt_classification  0.999975  0.000000   \n",
       "14       functions             anomaly_detection  0.999080  0.872852   \n",
       "15          motifs                    next_event  0.999602  0.948718   \n",
       "16          motifs  code_activity_classification  0.999602  0.948718   \n",
       "17          motifs     multi_file_classification  0.999602  0.948718   \n",
       "18          motifs         prompt_classification  0.999975  0.000000   \n",
       "19          motifs             anomaly_detection  0.999602  0.948718   \n",
       "20             raw                    next_event  0.999677  0.958730   \n",
       "21             raw  code_activity_classification  0.999677  0.958730   \n",
       "22             raw     multi_file_classification  0.999677  0.958730   \n",
       "23             raw         prompt_classification  0.999975  0.000000   \n",
       "24             raw             anomaly_detection  0.999677  0.958730   \n",
       "25          tokens             context_retrieval       NaN       NaN   \n",
       "26  semantic_edits             context_retrieval       NaN       NaN   \n",
       "27       functions             context_retrieval       NaN       NaN   \n",
       "28          motifs             context_retrieval       NaN       NaN   \n",
       "29             raw             context_retrieval       NaN       NaN   \n",
       "\n",
       "    train_time  infer_time  storage_bytes  avg_terms  recall@1  recall@5  \\\n",
       "0     0.340506    0.008164      16.693374   1.818207       NaN       NaN   \n",
       "1     0.120972    0.009219      16.693374   1.818207       NaN       NaN   \n",
       "2     0.162775    0.006980      16.693374   1.818207       NaN       NaN   \n",
       "3     0.190184    0.023912      16.693374   1.818207       NaN       NaN   \n",
       "4     0.197283    0.013617      16.693374   1.818207       NaN       NaN   \n",
       "5     0.451204    0.025396      15.896739   1.148765       NaN       NaN   \n",
       "6     0.467872    0.028396      15.896739   1.148765       NaN       NaN   \n",
       "7     0.654524    0.063071      15.896739   1.148765       NaN       NaN   \n",
       "8     0.495498    0.054430      15.896739   1.148765       NaN       NaN   \n",
       "9     0.888593    0.067836      15.896739   1.148765       NaN       NaN   \n",
       "10    0.233073    0.012403      12.371539   1.135991       NaN       NaN   \n",
       "11    0.201618    0.012878      12.371539   1.135991       NaN       NaN   \n",
       "12    0.141306    0.012997      12.371539   1.135991       NaN       NaN   \n",
       "13    0.218365    0.029164      12.371539   1.135991       NaN       NaN   \n",
       "14    0.246301    0.012389      12.371539   1.135991       NaN       NaN   \n",
       "15    0.111183    0.007152      14.278866   1.035290       NaN       NaN   \n",
       "16    0.118419    0.005044      14.278866   1.035290       NaN       NaN   \n",
       "17    0.365719    0.018551      14.278866   1.035290       NaN       NaN   \n",
       "18    0.098018    0.007322      14.278866   1.035290       NaN       NaN   \n",
       "19    0.157488    0.009276      14.278866   1.035290       NaN       NaN   \n",
       "20    1.544939    0.164612     641.943238  74.039540       NaN       NaN   \n",
       "21    1.648676    0.127795     641.943238  74.039540       NaN       NaN   \n",
       "22    1.717060    0.100346     641.943238  74.039540       NaN       NaN   \n",
       "23    1.703527    0.091919     641.943238  74.039540       NaN       NaN   \n",
       "24    1.747743    0.119874     641.943238  74.039540       NaN       NaN   \n",
       "25         NaN         NaN      16.693374   1.818207  0.996173  0.996695   \n",
       "26         NaN         NaN      15.896739   1.148765  0.996744  0.997490   \n",
       "27         NaN         NaN      12.371539   1.135991  0.001168  0.997217   \n",
       "28         NaN         NaN      14.278866   1.035290  0.995676  0.996695   \n",
       "29         NaN         NaN     641.943238  74.039540  0.996720  0.997415   \n",
       "\n",
       "    recall@10     latency  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  \n",
       "5         NaN         NaN  \n",
       "6         NaN         NaN  \n",
       "7         NaN         NaN  \n",
       "8         NaN         NaN  \n",
       "9         NaN         NaN  \n",
       "10        NaN         NaN  \n",
       "11        NaN         NaN  \n",
       "12        NaN         NaN  \n",
       "13        NaN         NaN  \n",
       "14        NaN         NaN  \n",
       "15        NaN         NaN  \n",
       "16        NaN         NaN  \n",
       "17        NaN         NaN  \n",
       "18        NaN         NaN  \n",
       "19        NaN         NaN  \n",
       "20        NaN         NaN  \n",
       "21        NaN         NaN  \n",
       "22        NaN         NaN  \n",
       "23        NaN         NaN  \n",
       "24        NaN         NaN  \n",
       "25   0.997043   46.699182  \n",
       "26   0.997639   64.151296  \n",
       "27   0.997341   86.988718  \n",
       "28   0.996993  102.346322  \n",
       "29   0.997614  361.740176  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieval_metrics(rung: str, ks=(1, 5, 10)):\n",
    "    reps = [entry[rung] for entry in features]\n",
    "    targets = [entry['target_file'] for entry in features]\n",
    "    placeholder = 'empty_representation'\n",
    "    clean_reps = [rep if rep.strip() else placeholder for rep in reps]\n",
    "    vec = TfidfVectorizer(max_features=4096)\n",
    "    matrix = vec.fit_transform(clean_reps)\n",
    "    sims = cosine_similarity(matrix)\n",
    "    np.fill_diagonal(sims, -np.inf)\n",
    "    results = {k: 0 for k in ks}\n",
    "    for idx, target in enumerate(targets):\n",
    "        ranks = np.argsort(-sims[idx])\n",
    "        for k in ks:\n",
    "            if any(targets[pos] == target for pos in ranks[:k]):\n",
    "                results[k] += 1\n",
    "    total = len(targets)\n",
    "    return {f'recall@{k}': results[k] / total for k in ks}\n",
    "retrieval_stats = []\n",
    "for rung in RUNG_FUNCS:\n",
    "    start = time.perf_counter()\n",
    "    recs = retrieval_metrics(rung)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    recs.update({\n",
    "        'rung': rung,\n",
    "        'task': 'context_retrieval',\n",
    "        'latency': elapsed,\n",
    "        'storage_bytes': np.mean([entry['storage_bytes'][rung] for entry in features]),\n",
    "        'avg_terms': np.mean([entry['term_count'][rung] for entry in features])\n",
    "    })\n",
    "    retrieval_stats.append(recs)\n",
    "df_retrieval = pd.DataFrame(retrieval_stats)\n",
    "df_final = pd.concat([df_probes, df_retrieval], ignore_index=True, sort=False)\n",
    "df_final.to_csv(REPO_ROOT / 'research/results/probe_metrics.csv', index=False)\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8ffc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (755139599.py, line 87)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mvec.fit_transform(cleaned)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Parsing and Interpretation\n",
    "# ============================================================================\n",
    "# Analyze the dataset to understand why certain rungs perform better/worse\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET PARSING AND INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Representation Characteristics Analysis\n",
    "print(\"\\n1. REPRESENTATION CHARACTERISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "rung_stats = {}\n",
    "for rung in RUNG_FUNCS.keys():\n",
    "    reps = [entry[rung] for entry in features]\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    all_terms = []\n",
    "    for rep in reps:\n",
    "        if rep and rep.strip():\n",
    "            terms = rep.split()\n",
    "            all_terms.extend(terms)\n",
    "    \n",
    "    vocab = Counter(all_terms)\n",
    "    unique_terms = len(vocab)\n",
    "    total_terms = len(all_terms)\n",
    "    avg_length = np.mean([len(rep.split()) for rep in reps if rep.strip()])\n",
    "    \n",
    "    # Diversity metrics\n",
    "    if total_terms > 0:\n",
    "        diversity = unique_terms / total_terms  # Type-token ratio\n",
    "        most_common = vocab.most_common(10)\n",
    "    else:\n",
    "        diversity = 0\n",
    "        most_common = []\n",
    "    \n",
    "    rung_stats[rung] = {\n",
    "        'vocab_size': unique_terms,\n",
    "        'total_terms': total_terms,\n",
    "        'avg_length': avg_length,\n",
    "        'diversity': diversity,\n",
    "        'most_common': most_common,\n",
    "        'empty_reprs': sum(1 for rep in reps if not rep or not rep.strip()),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{rung.upper()}:\")\n",
    "    print(f\"  Vocabulary size: {unique_terms}\")\n",
    "    print(f\"  Total terms: {total_terms}\")\n",
    "    print(f\"  Avg representation length: {avg_length:.1f} terms\")\n",
    "    print(f\"  Diversity (type-token ratio): {diversity:.3f}\")\n",
    "    print(f\"  Empty representations: {rung_stats[rung]['empty_reprs']} ({rung_stats[rung]['empty_reprs']/len(reps)*100:.1f}%)\")\n",
    "    if most_common:\n",
    "        print(f\"  Top 10 most common terms:\")\n",
    "        for term, count in most_common:\n",
    "            print(f\"    '{term}': {count} ({count/total_terms*100:.1f}%)\")\n",
    "\n",
    "# 2. Feature Importance Analysis (for successful classifications)\n",
    "print(\"\\n\\n2. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze which features are most important for each successful task\n",
    "for rung in RUNG_FUNCS.keys():\n",
    "    for label_name, task in task_labels:\n",
    "        if label_name not in ['high_code_activity', 'multi_file_session']:  # Focus on balanced tasks\n",
    "            continue\n",
    "        \n",
    "        X, y = build_probe_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=500)\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        # Get feature importance (coefficient magnitude)\n",
    "        feature_importance = np.abs(clf.coef_[0])\n",
    "        top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
    "        \n",
    "        # Get feature names from vectorizer\n",
    "        reps = [entry[rung] for entry in features]\n",
    "        cleaned = [repr if repr.strip() else 'empty_repr' for repr in reps]\n",
    "        vec = TfidfVectorizer(max_features=4096)\n",
    "\n",
    "            vec.fit_transform(cleaned)\n",
    "            feature_names = vec.get_feature_names_out()\n",
    "        \n",
    "        print(f\"\\n{rung} / {task}:\")\n",
    "        print(f\"  Accuracy: {accuracy_score(y, clf.predict(X)):.4f}\")\n",
    "        print(f\"  F1: {f1_score(y, clf.predict(X)):.4f}\")\n",
    "        print(f\"  Top 10 most important features:\")\n",
    "        for idx in top_indices[:10]:\n",
    "            if idx < len(feature_names):\n",
    "                importance = feature_importance[idx]\n",
    "                feature_name = feature_names[idx]\n",
    "                print(f\"    '{feature_name}': {importance:.4f}\")\n",
    "\n",
    "# 3. Representation Overlap Analysis\n",
    "print(\"\\n\\n3. REPRESENTATION OVERLAP ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"How much do representations overlap across rungs?\")\n",
    "\n",
    "# Sample a few traces and show their representations\n",
    "print(\"\\nSample trace representations (first 3 traces):\")\n",
    "for i in range(min(3, len(features))):\n",
    "    print(f\"\\nTrace {i+1}:\")\n",
    "    for rung in RUNG_FUNCS.keys():\n",
    "        rep = features[i][rung]\n",
    "        preview = rep[:200] + \"...\" if len(rep) > 200 else rep\n",
    "        print(f\"  {rung:15}: {preview}\")\n",
    "\n",
    "# 4. Performance vs Representation Characteristics\n",
    "print(\"\\n\\n4. PERFORMANCE VS REPRESENTATION CHARACTERISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "for rung in RUNG_FUNCS.keys():\n",
    "    stats = rung_stats[rung]\n",
    "    \n",
    "    # Get performance metrics\n",
    "    perf_data = df_final[df_final['rung'] == rung]\n",
    "    if not perf_data.empty:\n",
    "        avg_acc = perf_data['accuracy'].mean() if 'accuracy' in perf_data.columns else np.nan\n",
    "        avg_f1 = perf_data['f1'].mean() if 'f1' in perf_data.columns else np.nan\n",
    "        avg_recall1 = perf_data['recall@1'].mean() if 'recall@1' in perf_data.columns else np.nan\n",
    "    else:\n",
    "        avg_acc = avg_f1 = avg_recall1 = np.nan\n",
    "    \n",
    "    summary_data.append({\n",
    "        'rung': rung,\n",
    "        'vocab_size': stats['vocab_size'],\n",
    "        'avg_length': stats['avg_length'],\n",
    "        'diversity': stats['diversity'],\n",
    "        'storage_bytes': stats.get('storage_bytes', np.nan),\n",
    "        'avg_accuracy': avg_acc,\n",
    "        'avg_f1': avg_f1,\n",
    "        'avg_recall@1': avg_recall1,\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# 5. Error Analysis\n",
    "print(\"\\n\\n5. ERROR ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rung in RUNG_FUNCS.keys():\n",
    "    for label_name, task in task_labels:\n",
    "        if label_name not in ['high_code_activity', 'multi_file_session']:\n",
    "            continue\n",
    "        \n",
    "        X, y = build_probe_dataset(rung, label_name)\n",
    "        if len(np.unique(y)) < 2:\n",
    "            continue\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=500)\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X)\n",
    "        \n",
    "        # Find misclassified examples\n",
    "        errors = np.where(y != y_pred)[0]\n",
    "        if len(errors) > 0:\n",
    "            print(f\"\\n{rung} / {task}: {len(errors)} errors ({len(errors)/len(y)*100:.1f}%)\")\n",
    "            # Show a few error examples\n",
    "            for err_idx in errors[:3]:\n",
    "                print(f\"  Error example {err_idx}:\")\n",
    "                print(f\"    True label: {y[err_idx]}, Predicted: {y_pred[err_idx]}\")\n",
    "                rep = features[err_idx][rung]\n",
    "                preview = rep[:150] + \"...\" if len(rep) > 150 else rep\n",
    "                print(f\"    Representation: {preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Dataset parsing complete\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Performance Analysis (Altair) - OPTIMIZED\n",
    "# ============================================================================\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Enable high-resolution export\n",
    "alt.renderers.enable('default')\n",
    "alt.data_transformers.enable('default')\n",
    "\n",
    "# Shared chart configuration for better readability\n",
    "chart_config = {\n",
    "    'background': 'white',\n",
    "    'padding': {'left': 15, 'top': 15, 'right': 15, 'bottom': 15},\n",
    "    'view': {'continuousWidth': 400, 'continuousHeight': 300}\n",
    "}\n",
    "\n",
    "def configure_chart(chart, title_size=18, axis_label_size=12, axis_title_size=14):\n",
    "    \"\"\"Configure chart with consistent styling.\"\"\"\n",
    "    return chart.configure(**chart_config).configure_title(\n",
    "        fontSize=title_size,\n",
    "        fontWeight='bold',\n",
    "        anchor='start',\n",
    "        offset=10\n",
    "    ).configure_axis(\n",
    "        labelFontSize=axis_label_size,\n",
    "        titleFontSize=axis_title_size,\n",
    "        titleFontWeight='bold',\n",
    "        labelAngle=0\n",
    "    ).configure_legend(\n",
    "        labelFontSize=11,\n",
    "        titleFontSize=12,\n",
    "        titleFontWeight='bold'\n",
    "    )\n",
    "\n",
    "# Filter out NaN values for classification tasks\n",
    "class_df = df_final[df_final['task'].isin(['code_activity_classification', 'multi_file_classification', 'anomaly_detection'])].copy()\n",
    "class_df = class_df.dropna(subset=['accuracy', 'f1'])\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Performance Comparison Charts\n",
    "# ============================================================================\n",
    "\n",
    "if not class_df.empty:\n",
    "    # Prepare data for accuracy plot\n",
    "    pivot_acc = class_df.pivot_table(values='accuracy', index='rung', columns='task', aggfunc='mean').reset_index()\n",
    "    pivot_acc_melted = pivot_acc.melt(id_vars='rung', var_name='task', value_name='accuracy')\n",
    "    pivot_acc_melted['task'] = pivot_acc_melted['task'].str.replace('_', ' ').str.title()\n",
    "    \n",
    "    # Chart 1: Accuracy by Rung and Task (OPTIMIZED - larger, better colors)\n",
    "    chart_acc = alt.Chart(pivot_acc_melted).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'], \n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('accuracy:Q', title='Accuracy', scale=alt.Scale(domain=[0, 1.05])),\n",
    "        color=alt.Color('task:N', \n",
    "                       scale=alt.Scale(domain=['Code Activity Classification', 'Multi File Classification', 'Anomaly Detection'],\n",
    "                                      range=['#82a7a6', '#b57c61', '#73648a']),\n",
    "                       legend=alt.Legend(title='Task', orient='right', columns=1)),\n",
    "        tooltip=['rung', 'task', alt.Tooltip('accuracy:Q', format='.3f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Classification Accuracy by Rung'\n",
    "    )\n",
    "    \n",
    "    chart_acc_configured = configure_chart(chart_acc)\n",
    "    chart_acc_configured.save(REPO_ROOT / 'research/results/classification_accuracy_by_rung.png', scale_factor=3)\n",
    "    print(\"✓ Saved classification_accuracy_by_rung.png\")\n",
    "    \n",
    "    # Chart 2: F1 Score by Rung and Task (OPTIMIZED)\n",
    "    pivot_f1 = class_df.pivot_table(values='f1', index='rung', columns='task', aggfunc='mean').reset_index()\n",
    "    pivot_f1_melted = pivot_f1.melt(id_vars='rung', var_name='task', value_name='f1')\n",
    "    pivot_f1_melted['task'] = pivot_f1_melted['task'].str.replace('_', ' ').str.title()\n",
    "    \n",
    "    chart_f1 = alt.Chart(pivot_f1_melted).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('f1:Q', title='F1 Score', scale=alt.Scale(domain=[0, 1.05])),\n",
    "        color=alt.Color('task:N',\n",
    "                       scale=alt.Scale(domain=['Code Activity Classification', 'Multi File Classification', 'Anomaly Detection'],\n",
    "                                      range=['#82a7a6', '#b57c61', '#73648a']),\n",
    "                       legend=alt.Legend(title='Task', orient='right', columns=1)),\n",
    "        tooltip=['rung', 'task', alt.Tooltip('f1:Q', format='.3f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='F1 Score by Rung'\n",
    "    )\n",
    "    \n",
    "    chart_f1_configured = configure_chart(chart_f1)\n",
    "    chart_f1_configured.save(REPO_ROOT / 'research/results/f1_score_by_rung.png', scale_factor=3)\n",
    "    print(\"✓ Saved f1_score_by_rung.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Context Retrieval Performance\n",
    "# ============================================================================\n",
    "\n",
    "retrieval_df = df_final[df_final['task'] == 'context_retrieval'].dropna(subset=['recall@1', 'recall@5', 'recall@10']).copy()\n",
    "if not retrieval_df.empty:\n",
    "    retrieval_melted = retrieval_df.melt(\n",
    "        id_vars='rung',\n",
    "        value_vars=['recall@1', 'recall@5', 'recall@10'],\n",
    "        var_name='metric',\n",
    "        value_name='recall'\n",
    "    )\n",
    "    retrieval_melted['metric'] = retrieval_melted['metric'].str.replace('recall@', 'Recall@')\n",
    "    \n",
    "    chart_retrieval = alt.Chart(retrieval_melted).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('recall:Q', title='Recall', scale=alt.Scale(domain=[0, 1])),\n",
    "        color=alt.Color('metric:N',\n",
    "                       scale=alt.Scale(domain=['Recall@1', 'Recall@5', 'Recall@10'],\n",
    "                                      range=['#82a7a6', '#b57c61', '#73648a']),\n",
    "                       legend=alt.Legend(title='Metric', orient='right')),\n",
    "        tooltip=['rung', 'metric', alt.Tooltip('recall:Q', format='.3f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Context Retrieval Performance'\n",
    "    )\n",
    "    \n",
    "    chart_retrieval_configured = configure_chart(chart_retrieval)\n",
    "    chart_retrieval_configured.save(REPO_ROOT / 'research/results/context_retrieval_performance.png', scale_factor=3)\n",
    "    print(\"✓ Saved context_retrieval_performance.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Storage Efficiency vs Performance Trade-off\n",
    "# ============================================================================\n",
    "\n",
    "if not class_df.empty:\n",
    "    task_df = class_df[class_df['task'] == 'code_activity_classification'].dropna(subset=['accuracy', 'storage_bytes']).copy()\n",
    "    if not task_df.empty:\n",
    "        chart_tradeoff = alt.Chart(task_df).mark_circle(size=300, opacity=0.8, stroke='black', strokeWidth=1.5).encode(\n",
    "            x=alt.X('storage_bytes:Q', title='Storage (bytes)', scale=alt.Scale(type='log', base=10),\n",
    "                    axis=alt.Axis(format='.0e')),\n",
    "            y=alt.Y('accuracy:Q', title='Accuracy', scale=alt.Scale(domain=[0.65, 1.05])),\n",
    "            color=alt.Color('rung:N', \n",
    "                           scale=alt.Scale(domain=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                                          range=['#82a7a6', '#b57c61', '#73648a', '#5c4e6d', '#453750']),\n",
    "                           legend=alt.Legend(title='Rung', orient='right')),\n",
    "            tooltip=['rung', alt.Tooltip('accuracy:Q', format='.3f'), alt.Tooltip('storage_bytes:Q', format=',.0f')]\n",
    "        ).properties(\n",
    "            width=500,\n",
    "            height=350,\n",
    "            title='Storage vs Accuracy Trade-off'\n",
    "        )\n",
    "        \n",
    "        chart_tradeoff_configured = configure_chart(chart_tradeoff)\n",
    "        chart_tradeoff_configured.save(REPO_ROOT / 'research/results/storage_vs_accuracy_tradeoff.png', scale_factor=3)\n",
    "        print(\"✓ Saved storage_vs_accuracy_tradeoff.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Representation Characteristics Visualization\n",
    "# ============================================================================\n",
    "\n",
    "if 'df_summary' in locals() and not df_summary.empty:\n",
    "    # Chart 1: Vocabulary Size\n",
    "    chart_vocab = alt.Chart(df_summary).mark_bar(color='#82a7a6', opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('vocab_size:Q', title='Unique Terms', scale=alt.Scale(type='log', base=10)),\n",
    "        tooltip=['rung', alt.Tooltip('vocab_size:Q', format=',.0f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Vocabulary Size by Rung'\n",
    "    )\n",
    "    \n",
    "    chart_vocab_configured = configure_chart(chart_vocab)\n",
    "    chart_vocab_configured.save(REPO_ROOT / 'research/results/vocabulary_size_by_rung.png', scale_factor=3)\n",
    "    print(\"✓ Saved vocabulary_size_by_rung.png\")\n",
    "    \n",
    "    # Chart 2: Average Representation Length\n",
    "    chart_avg_length = alt.Chart(df_summary).mark_bar(color='#b57c61', opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('avg_length:Q', title='Avg Terms per Representation'),\n",
    "        tooltip=['rung', alt.Tooltip('avg_length:Q', format='.1f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Average Representation Length'\n",
    "    )\n",
    "    \n",
    "    chart_avg_length_configured = configure_chart(chart_avg_length)\n",
    "    chart_avg_length_configured.save(REPO_ROOT / 'research/results/avg_representation_length.png', scale_factor=3)\n",
    "    print(\"✓ Saved avg_representation_length.png\")\n",
    "    \n",
    "    # Chart 3: Diversity (Type-Token Ratio)\n",
    "    chart_diversity = alt.Chart(df_summary).mark_bar(color='#73648a', opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('rung:N', title='Abstraction Rung', sort=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y('diversity:Q', title='Type-Token Ratio'),\n",
    "        tooltip=['rung', alt.Tooltip('diversity:Q', format='.3f')]\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Representation Diversity'\n",
    "    )\n",
    "    \n",
    "    chart_diversity_configured = configure_chart(chart_diversity)\n",
    "    chart_diversity_configured.save(REPO_ROOT / 'research/results/representation_diversity.png', scale_factor=3)\n",
    "    print(\"✓ Saved representation_diversity.png\")\n",
    "    \n",
    "    # Chart 4: Performance vs Diversity\n",
    "    task_df = class_df[class_df['task'] == 'code_activity_classification'].dropna(subset=['accuracy']).copy()\n",
    "    if not task_df.empty and 'diversity' in df_summary.columns:\n",
    "        merged = task_df.merge(df_summary[['rung', 'diversity']], on='rung', how='left')\n",
    "        merged = merged.dropna(subset=['accuracy', 'diversity'])\n",
    "        if not merged.empty:\n",
    "            chart_diversity_acc = alt.Chart(merged).mark_circle(size=300, opacity=0.8, stroke='black', strokeWidth=1.5).encode(\n",
    "                x=alt.X('diversity:Q', title='Diversity (Type-Token Ratio)'),\n",
    "                y=alt.Y('accuracy:Q', title='Accuracy', scale=alt.Scale(domain=[0.65, 1.05])),\n",
    "                color=alt.Color('rung:N', \n",
    "                               scale=alt.Scale(domain=['tokens', 'semantic_edits', 'functions', 'motifs', 'raw'],\n",
    "                                              range=['#82a7a6', '#b57c61', '#73648a', '#5c4e6d', '#453750']),\n",
    "                               legend=alt.Legend(title='Rung', orient='right')),\n",
    "                tooltip=['rung', alt.Tooltip('diversity:Q', format='.3f'), alt.Tooltip('accuracy:Q', format='.3f')]\n",
    "            ).properties(\n",
    "                width=500,\n",
    "                height=350,\n",
    "                title='Diversity vs Accuracy'\n",
    "            )\n",
    "            \n",
    "            chart_diversity_acc_configured = configure_chart(chart_diversity_acc)\n",
    "            chart_diversity_acc_configured.save(REPO_ROOT / 'research/results/diversity_vs_accuracy.png', scale_factor=3)\n",
    "            print(\"✓ Saved diversity_vs_accuracy.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Inter-Probe Difference Visualizations (NEW)\n",
    "# ============================================================================\n",
    "\n",
    "if 'df_inter_probe' in locals() and not df_inter_probe.empty:\n",
    "    # Chart 1: Inter-Probe Difference by Task and Metric\n",
    "    chart_inter_probe = alt.Chart(df_inter_probe).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('task:N', title='Task', axis=alt.Axis(labelAngle=-45)),\n",
    "        y=alt.Y('difference:Q', title='Inter-Probe Difference (Max - Min Performance)'),\n",
    "        color=alt.Color('metric:N',\n",
    "                       scale=alt.Scale(domain=['accuracy', 'f1', 'recall@1', 'recall@5', 'recall@10'],\n",
    "                                      range=['#82a7a6', '#b57c61', '#73648a', '#5c4e6d', '#453750']),\n",
    "                       legend=alt.Legend(title='Metric', orient='right')),\n",
    "        tooltip=['task', 'metric', alt.Tooltip('difference:Q', format='.3f'), \n",
    "                 alt.Tooltip('best_rung:N', title='Best Rung'), alt.Tooltip('worst_rung:N', title='Worst Rung')]\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        title='Inter-Probe Difference by Task and Metric'\n",
    "    )\n",
    "    \n",
    "    chart_inter_probe_configured = configure_chart(chart_inter_probe)\n",
    "    chart_inter_probe_configured.save(REPO_ROOT / 'research/results/inter_probe_difference_by_task.png', scale_factor=3)\n",
    "    print(\"✓ Saved inter_probe_difference_by_task.png\")\n",
    "    \n",
    "    # Chart 2: Best vs Worst Rung Performance\n",
    "    inter_probe_melted = df_inter_probe.melt(\n",
    "        id_vars=['task', 'metric', 'best_rung', 'worst_rung'],\n",
    "        value_vars=['max_performance', 'min_performance'],\n",
    "        var_name='performance_type',\n",
    "        value_name='performance'\n",
    "    )\n",
    "    inter_probe_melted['performance_type'] = inter_probe_melted['performance_type'].str.replace('_', ' ').str.title()\n",
    "    \n",
    "    chart_best_worst = alt.Chart(inter_probe_melted).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('task:N', title='Task', axis=alt.Axis(labelAngle=-45)),\n",
    "        y=alt.Y('performance:Q', title='Performance', scale=alt.Scale(domain=[0, 1.05])),\n",
    "        color=alt.Color('performance_type:N',\n",
    "                       scale=alt.Scale(domain=['Max Performance', 'Min Performance'],\n",
    "                                      range=['#82a7a6', '#73648a']),\n",
    "                       legend=alt.Legend(title='Performance Type', orient='right')),\n",
    "        column=alt.Column('metric:N', title='Metric', header=alt.Header(labelAngle=-45)),\n",
    "        tooltip=['task', 'metric', 'performance_type', alt.Tooltip('performance:Q', format='.3f')]\n",
    "    ).properties(\n",
    "        width=200,\n",
    "        height=300,\n",
    "        title='Best vs Worst Rung Performance'\n",
    "    )\n",
    "    \n",
    "    chart_best_worst_configured = configure_chart(chart_best_worst, title_size=16)\n",
    "    chart_best_worst_configured.save(REPO_ROOT / 'research/results/best_worst_rung_performance.png', scale_factor=3)\n",
    "    print(\"✓ Saved best_worst_rung_performance.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Rung Selection Heuristic Visualization (NEW)\n",
    "# ============================================================================\n",
    "\n",
    "if 'df_heuristic' in locals() and not df_heuristic.empty:\n",
    "    # Add match indicator color\n",
    "    df_heuristic_viz = df_heuristic.copy()\n",
    "    df_heuristic_viz['match_color'] = df_heuristic_viz['matches'].apply(lambda x: '#82a7a6' if x == '✓' else '#5c4e6d')\n",
    "    \n",
    "    chart_heuristic = alt.Chart(df_heuristic_viz).mark_bar(opacity=0.85, cornerRadius=5, stroke='white', strokeWidth=1).encode(\n",
    "        x=alt.X('task:N', title='Task', axis=alt.Axis(labelAngle=-45)),\n",
    "        y=alt.Y('confidence:Q', title='Confidence', scale=alt.Scale(domain=[0, 1])),\n",
    "        color=alt.Color('matches:N',\n",
    "                       scale=alt.Scale(domain=['✓', '✗'],\n",
    "                                      range=['#82a7a6', '#5c4e6d']),\n",
    "                       legend=alt.Legend(title='Match', orient='right')),\n",
    "        tooltip=['task', 'selected_rung', 'actual_best_rung', 'matches', \n",
    "                 alt.Tooltip('confidence:Q', format='.2f'), 'reasoning']\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=350,\n",
    "        title='Rung Selection Heuristic: Confidence and Accuracy'\n",
    "    )\n",
    "    \n",
    "    chart_heuristic_configured = configure_chart(chart_heuristic)\n",
    "    chart_heuristic_configured.save(REPO_ROOT / 'research/results/rung_selection_heuristic.png', scale_factor=3)\n",
    "    print(\"✓ Saved rung_selection_heuristic.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Create Combined HTML Dashboards\n",
    "# ============================================================================\n",
    "\n",
    "# Performance dashboard\n",
    "if not class_df.empty and not retrieval_df.empty:\n",
    "    top_row = alt.hconcat(chart_acc, chart_f1, spacing=30)\n",
    "    bottom_row = alt.hconcat(chart_retrieval, chart_tradeoff, spacing=30)\n",
    "    combined = alt.vconcat(top_row, bottom_row, spacing=40).configure(**chart_config).configure_title(\n",
    "        fontSize=18, fontWeight='bold', anchor='start', offset=10\n",
    "    ).configure_axis(labelFontSize=12, titleFontSize=14, titleFontWeight='bold')\n",
    "    \n",
    "    combined.save(REPO_ROOT / 'research/results/probe_performance_analysis.html')\n",
    "    print(\"✓ Saved interactive HTML to research/results/probe_performance_analysis.html\")\n",
    "\n",
    "# Representation characteristics dashboard\n",
    "if 'df_summary' in locals() and not df_summary.empty:\n",
    "    top_row_char = alt.hconcat(chart_vocab, chart_avg_length, spacing=30)\n",
    "    bottom_row_char = alt.hconcat(chart_diversity, chart_diversity_acc, spacing=30)\n",
    "    combined_char = alt.vconcat(top_row_char, bottom_row_char, spacing=40).configure(**chart_config).configure_title(\n",
    "        fontSize=18, fontWeight='bold', anchor='start', offset=10\n",
    "    ).configure_axis(labelFontSize=12, titleFontSize=14, titleFontWeight='bold')\n",
    "    \n",
    "    combined_char.save(REPO_ROOT / 'research/results/representation_characteristics.html')\n",
    "    print(\"✓ Saved interactive HTML to research/results/representation_characteristics.html\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Copy images to landing page\n",
    "# ============================================================================\n",
    "\n",
    "LANDING_IMAGES_DIR = REPO_ROOT / 'telemetry-landing/public/images'\n",
    "LANDING_IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "images_to_copy = [\n",
    "    'classification_accuracy_by_rung.png',\n",
    "    'f1_score_by_rung.png',\n",
    "    'context_retrieval_performance.png',\n",
    "    'storage_vs_accuracy_tradeoff.png',\n",
    "    'vocabulary_size_by_rung.png',\n",
    "    'representation_diversity.png',\n",
    "    'inter_probe_difference_by_task.png',\n",
    "    'rung_selection_heuristic.png',\n",
    "]\n",
    "for img_name in images_to_copy:\n",
    "    src = REPO_ROOT / 'research/results' / img_name\n",
    "    dst = LANDING_IMAGES_DIR / img_name\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'✓ Copied {img_name} to landing page')\n",
    "    else:\n",
    "        print(f'⚠ {img_name} not found - run cell to generate')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All visualizations generated and saved!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1891e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "834f6b59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afa8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Detailed Results Interpretation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED RESULTS INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive interpretation\n",
    "interpretation = []\n",
    "\n",
    "for rung in RUNG_FUNCS.keys():\n",
    "    rung_data = df_final[df_final['rung'] == rung]\n",
    "    \n",
    "    # Classification performance\n",
    "    class_data = rung_data[rung_data['task'].isin(['code_activity_classification', 'multi_file_classification'])]\n",
    "    if not class_data.empty:\n",
    "        avg_acc = class_data['accuracy'].mean()\n",
    "        avg_f1 = class_data['f1'].mean()\n",
    "    else:\n",
    "        avg_acc = avg_f1 = np.nan\n",
    "    \n",
    "    # Retrieval performance\n",
    "    retrieval_data = rung_data[rung_data['task'] == 'context_retrieval']\n",
    "    if not retrieval_data.empty:\n",
    "        recall1 = retrieval_data['recall@1'].iloc[0] if 'recall@1' in retrieval_data.columns else np.nan\n",
    "        recall5 = retrieval_data['recall@5'].iloc[0] if 'recall@5' in retrieval_data.columns else np.nan\n",
    "        recall10 = retrieval_data['recall@10'].iloc[0] if 'recall@10' in retrieval_data.columns else np.nan\n",
    "    else:\n",
    "        recall1 = recall5 = recall10 = np.nan\n",
    "    \n",
    "    # Storage\n",
    "    storage = rung_data['storage_bytes'].iloc[0] if 'storage_bytes' in rung_data.columns else np.nan\n",
    "    terms = rung_data['avg_terms'].iloc[0] if 'avg_terms' in rung_data.columns else np.nan\n",
    "    \n",
    "    # Stats\n",
    "    stats = rung_stats.get(rung, {})\n",
    "    \n",
    "    interpretation.append({\n",
    "        'rung': rung,\n",
    "        'classification_accuracy': avg_acc,\n",
    "        'classification_f1': avg_f1,\n",
    "        'retrieval_recall@1': recall1,\n",
    "        'retrieval_recall@5': recall5,\n",
    "        'retrieval_recall@10': recall10,\n",
    "        'storage_bytes': storage,\n",
    "        'avg_terms': terms,\n",
    "        'vocab_size': stats.get('vocab_size', np.nan),\n",
    "        'diversity': stats.get('diversity', np.nan),\n",
    "    })\n",
    "\n",
    "df_interpretation = pd.DataFrame(interpretation)\n",
    "\n",
    "print(\"\\nComprehensive Performance Summary:\")\n",
    "print(df_interpretation.to_string(index=False))\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\\nKEY FINDINGS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find best performers\n",
    "best_class = df_interpretation.loc[df_interpretation['classification_f1'].idxmax()]\n",
    "best_retrieval = df_interpretation.loc[df_interpretation['retrieval_recall@5'].idxmax()]\n",
    "most_efficient = df_interpretation.loc[df_interpretation['storage_bytes'].idxmin()]\n",
    "\n",
    "print(f\"\\n1. Best Classification Performance:\")\n",
    "print(f\"   Rung: {best_class['rung']}\")\n",
    "print(f\"   Accuracy: {best_class['classification_accuracy']:.4f}\")\n",
    "print(f\"   F1: {best_class['classification_f1']:.4f}\")\n",
    "print(f\"   Storage: {best_class['storage_bytes']:.1f} bytes\")\n",
    "\n",
    "print(f\"\\n2. Best Retrieval Performance:\")\n",
    "print(f\"   Rung: {best_retrieval['rung']}\")\n",
    "print(f\"   Recall@1: {best_retrieval['retrieval_recall@1']:.4f}\")\n",
    "print(f\"   Recall@5: {best_retrieval['retrieval_recall@5']:.4f}\")\n",
    "print(f\"   Recall@10: {best_retrieval['retrieval_recall@10']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Most Storage Efficient:\")\n",
    "print(f\"   Rung: {most_efficient['rung']}\")\n",
    "print(f\"   Storage: {most_efficient['storage_bytes']:.1f} bytes\")\n",
    "print(f\"   Terms: {most_efficient['avg_terms']:.1f}\")\n",
    "print(f\"   Classification F1: {most_efficient['classification_f1']:.4f}\")\n",
    "print(f\"   Retrieval Recall@5: {most_efficient['retrieval_recall@5']:.4f}\")\n",
    "\n",
    "# Trade-off analysis\n",
    "print(f\"\\n4. Privacy-Utility Trade-off Analysis:\")\n",
    "print(f\"   {'Rung':<15} {'Classification':<15} {'Retrieval':<15} {'Storage':<12} {'Privacy Level'}\")\n",
    "print(f\"   {'-'*15} {'-'*15} {'-'*15} {'-'*12} {'-'*15}\")\n",
    "for _, row in df_interpretation.iterrows():\n",
    "    class_perf = f\"{row['classification_f1']:.3f}\" if not pd.isna(row['classification_f1']) else \"N/A\"\n",
    "    retrieval_perf = f\"{row['retrieval_recall@5']:.3f}\" if not pd.isna(row['retrieval_recall@5']) else \"N/A\"\n",
    "    storage = f\"{row['storage_bytes']:.0f}\" if not pd.isna(row['storage_bytes']) else \"N/A\"\n",
    "    \n",
    "    # Infer privacy level from storage (smaller = more private)\n",
    "    if not pd.isna(row['storage_bytes']):\n",
    "        if row['storage_bytes'] < 200:\n",
    "            privacy = \"High\"\n",
    "        elif row['storage_bytes'] < 500:\n",
    "            privacy = \"Medium\"\n",
    "        else:\n",
    "            privacy = \"Low\"\n",
    "    else:\n",
    "        privacy = \"Unknown\"\n",
    "    \n",
    "    print(f\"   {row['rung']:<15} {class_perf:<15} {retrieval_perf:<15} {storage:<12} {privacy}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b46fc9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Inter-Probe Difference Statistics\n",
    "# ============================================================================\n",
    "# Calculate how much performance varies across rungs for each task/metric\n",
    "# This helps determine if rung choice is critical\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INTER-PROBE DIFFERENCE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all metrics for each task/rung combination\n",
    "inter_probe_data = []\n",
    "\n",
    "# Classification metrics\n",
    "for task in ['code_activity_classification', 'multi_file_classification', 'anomaly_detection']:\n",
    "    task_data = df_final[df_final['task'] == task]\n",
    "    for metric in ['accuracy', 'f1']:\n",
    "        if metric in task_data.columns:\n",
    "            metric_values = task_data[metric].dropna()\n",
    "            if len(metric_values) > 0:\n",
    "                rung_perfs = task_data[['rung', metric]].dropna()\n",
    "                if len(rung_perfs) > 0:\n",
    "                    max_perf = rung_perfs[metric].max()\n",
    "                    min_perf = rung_perfs[metric].min()\n",
    "                    best_rung = rung_perfs.loc[rung_perfs[metric].idxmax(), 'rung']\n",
    "                    worst_rung = rung_perfs.loc[rung_perfs[metric].idxmin(), 'rung']\n",
    "                    diff = max_perf - min_perf\n",
    "                    std_val = rung_perfs[metric].std() if len(rung_perfs) > 1 else 0.0\n",
    "                    range_ratio = diff / max_perf if max_perf > 0 else 0.0\n",
    "                    \n",
    "                    inter_probe_data.append({\n",
    "                        'task': task,\n",
    "                        'metric': metric,\n",
    "                        'max_performance': max_perf,\n",
    "                        'min_performance': min_perf,\n",
    "                        'difference': diff,\n",
    "                        'range_ratio': range_ratio,\n",
    "                        'std': std_val,\n",
    "                        'best_rung': best_rung,\n",
    "                        'worst_rung': worst_rung,\n",
    "                        'num_rungs': len(rung_perfs)\n",
    "                    })\n",
    "\n",
    "# Retrieval metrics\n",
    "retrieval_task = 'context_retrieval'\n",
    "retrieval_data = df_final[df_final['task'] == retrieval_task]\n",
    "for metric in ['recall@1', 'recall@5', 'recall@10']:\n",
    "    if metric in retrieval_data.columns:\n",
    "        metric_values = retrieval_data[metric].dropna()\n",
    "        if len(metric_values) > 0:\n",
    "            rung_perfs = retrieval_data[['rung', metric]].dropna()\n",
    "            if len(rung_perfs) > 0:\n",
    "                max_perf = rung_perfs[metric].max()\n",
    "                min_perf = rung_perfs[metric].min()\n",
    "                best_rung = rung_perfs.loc[rung_perfs[metric].idxmax(), 'rung']\n",
    "                worst_rung = rung_perfs.loc[rung_perfs[metric].idxmin(), 'rung']\n",
    "                diff = max_perf - min_perf\n",
    "                std_val = rung_perfs[metric].std() if len(rung_perfs) > 1 else 0.0\n",
    "                range_ratio = diff / max_perf if max_perf > 0 else 0.0\n",
    "                \n",
    "                inter_probe_data.append({\n",
    "                    'task': retrieval_task,\n",
    "                    'metric': metric,\n",
    "                    'max_performance': max_perf,\n",
    "                    'min_performance': min_perf,\n",
    "                    'difference': diff,\n",
    "                    'range_ratio': range_ratio,\n",
    "                    'std': std_val,\n",
    "                    'best_rung': best_rung,\n",
    "                    'worst_rung': worst_rung,\n",
    "                    'num_rungs': len(rung_perfs)\n",
    "                })\n",
    "\n",
    "df_inter_probe = pd.DataFrame(inter_probe_data)\n",
    "print(\"\\nInter-Probe Difference by Task and Metric:\")\n",
    "print(df_inter_probe.to_string(index=False))\n",
    "\n",
    "# Save inter-probe difference statistics\n",
    "inter_probe_output_path = RESULTS_DIR / 'inter_probe_difference_stats.json'\n",
    "df_inter_probe.to_json(inter_probe_output_path, orient='records', indent=2)\n",
    "print(f\"\\n✓ Results saved to {inter_probe_output_path}\")\n",
    "df_inter_probe.to_csv(RESULTS_DIR / 'inter_probe_difference_stats.csv', index=False)\n",
    "print(f\"✓ Also saved as CSV to {RESULTS_DIR / 'inter_probe_difference_stats.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Rung Selection Heuristic\n",
    "# ============================================================================\n",
    "# Use inter-probe difference to automatically select the best rung for each task\n",
    "# High difference → rung choice is critical → use best-performing rung\n",
    "# Low difference → all rungs similar → use most efficient rung (motifs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNG SELECTION HEURISTIC RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define task types\n",
    "task_types = {\n",
    "    'code_activity_classification': 'classification',\n",
    "    'multi_file_classification': 'classification',\n",
    "    'anomaly_detection': 'other',\n",
    "    'context_retrieval': 'retrieval'\n",
    "}\n",
    "\n",
    "# Threshold for \"high\" vs \"low\" inter-probe difference\n",
    "DIFFERENCE_THRESHOLD = 0.15\n",
    "\n",
    "heuristic_results = []\n",
    "\n",
    "for task in task_types.keys():\n",
    "    task_type = task_types[task]\n",
    "    \n",
    "    # Get inter-probe difference for this task\n",
    "    task_diff_data = df_inter_probe[df_inter_probe['task'] == task]\n",
    "    \n",
    "    if len(task_diff_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use the primary metric for the task type\n",
    "    if task_type == 'classification':\n",
    "        # Use accuracy as primary metric\n",
    "        primary_metric = task_diff_data[task_diff_data['metric'] == 'accuracy']\n",
    "        if len(primary_metric) == 0:\n",
    "            primary_metric = task_diff_data[task_diff_data['metric'] == 'f1']\n",
    "    elif task_type == 'retrieval':\n",
    "        # Use recall@1 as primary metric\n",
    "        primary_metric = task_diff_data[task_diff_data['metric'] == 'recall@1']\n",
    "        if len(primary_metric) == 0:\n",
    "            primary_metric = task_diff_data[task_diff_data['metric'] == 'recall@5']\n",
    "    else:\n",
    "        # Use first available metric\n",
    "        primary_metric = task_diff_data.iloc[[0]]\n",
    "    \n",
    "    if len(primary_metric) == 0:\n",
    "        continue\n",
    "    \n",
    "    inter_probe_diff = primary_metric['difference'].iloc[0]\n",
    "    best_rung = primary_metric['best_rung'].iloc[0]\n",
    "    \n",
    "    # Heuristic logic\n",
    "    if inter_probe_diff >= DIFFERENCE_THRESHOLD:\n",
    "        # High difference → rung choice is critical → use best-performing rung\n",
    "        selected_rung = best_rung\n",
    "        confidence = 0.95\n",
    "        reasoning = f\"High inter-probe difference ({inter_probe_diff:.3f}) → rung choice critical → use {best_rung}\"\n",
    "    else:\n",
    "        # Low difference → all rungs similar → use most efficient rung (motifs)\n",
    "        selected_rung = 'motifs'  # Most compressed/efficient\n",
    "        confidence = 0.85\n",
    "        reasoning = f\"Low inter-probe difference ({inter_probe_diff:.3f}) → all rungs similar → use motifs (most efficient)\"\n",
    "    \n",
    "    # Validate against actual best rung\n",
    "    matches = \"✓\" if selected_rung == best_rung else \"✗\"\n",
    "    \n",
    "    heuristic_results.append({\n",
    "        'task': task,\n",
    "        'task_type': task_type,\n",
    "        'selected_rung': selected_rung,\n",
    "        'actual_best_rung': best_rung,\n",
    "        'matches': matches,\n",
    "        'confidence': confidence,\n",
    "        'reasoning': reasoning\n",
    "    })\n",
    "\n",
    "df_heuristic = pd.DataFrame(heuristic_results)\n",
    "print(\"\\nHeuristic Selection vs Actual Best Rung:\")\n",
    "print(df_heuristic.to_string(index=False))\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(1 for r in heuristic_results if r['matches'] == '✓')\n",
    "total = len(heuristic_results)\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "print(f\"\\n✓ Validation: {correct} correct, {total - correct} incorrect\")\n",
    "print(f\"  Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Save results\n",
    "heuristic_output_path = RESULTS_DIR / 'rung_selection_heuristic.json'\n",
    "df_heuristic.to_json(heuristic_output_path, orient='records', indent=2)\n",
    "print(f\"\\n✓ Results saved to {heuristic_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438282c2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a612d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa38bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a60068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06be4b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf16eb0",
   "metadata": {},
   "source": [
    "## Research Implications\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Task-Specific Rung Selection is Critical**\n",
    "   - Classification tasks → Motifs excel (100% accuracy, most efficient)\n",
    "   - Retrieval tasks → Semantic edits or functions (motifs too abstract)\n",
    "   - Tokens perform poorly across all tasks\n",
    "\n",
    "2. **Privacy-Utility Trade-off is Task-Dependent**\n",
    "   - Motifs: High privacy, excellent classification, poor retrieval\n",
    "   - Semantic edits: Lower privacy, good retrieval, good classification  \n",
    "   - Functions: Balanced privacy-utility for both tasks\n",
    "\n",
    "3. **Storage Efficiency Scales Dramatically**\n",
    "   - Motifs are 4-13x more efficient while achieving better classification\n",
    "   - Critical for large-scale deployment and foundation model training\n",
    "\n",
    "4. **Abstraction Can Improve Performance**\n",
    "   - Higher abstraction (motifs) captures structural patterns that generalize better\n",
    "   - Suggests structural patterns matter more than raw content for classification\n",
    "\n",
    "### Next Research Directions\n",
    "\n",
    "1. **Why Motifs Excel at Classification**\n",
    "   - Analyze which motifs are most predictive\n",
    "   - Understand structural pattern generalization\n",
    "\n",
    "2. **Why Tokens Perform Poorly**\n",
    "   - Investigate if structured token representations improve performance\n",
    "   - Compare with canonicalized vs raw tokens\n",
    "\n",
    "3. **Cross-Task Generalization**\n",
    "   - Test patterns across different classification tasks\n",
    "   - Identify retrieval tasks where motifs might work better\n",
    "\n",
    "4. **Privacy Quantification**\n",
    "   - Measure actual k-anonymity values\n",
    "   - Quantify re-identification risk\n",
    "\n",
    "5. **Foundation Model Training**\n",
    "   - Test if motifs can train better code models\n",
    "   - Validate storage efficiency enables larger datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
